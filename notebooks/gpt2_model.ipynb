{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2-model.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMzKYmPmdMeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "aebb61ee-d7f0-4e99-ed61-bf4c37d29907"
      },
      "source": [
        "!pip install gpt-2-simple\n",
        "!tar -xzf raw_corpus.tar.gz\n",
        "!mkdir /content/corpus\n",
        "!mv /content/*.txt /content/corpus/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/a9/e1545c0a96cfc5653ebb9e621cd00aca50ad0733a47e3f47ee211569fbd0/gpt_2_simple-0.5.4.tar.gz\n",
            "Collecting regex (from gpt-2-simple)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.16.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Building wheels for collected packages: gpt-2-simple, regex\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.5.4-cp36-none-any.whl size=25145 sha256=f215318ad55243ed4a562fad2855d05702fb00d11278aaf1274d2594df7c71d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/39/91/667e099cf36dee458e2b5e39fc202da34d2c02b4005dd5dad3\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.6.8-cp36-cp36m-linux_x86_64.whl size=604150 sha256=32b66aad354a63f03918a3cca4453ea09ef997b6f128c1852fe1b41dda559cfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built gpt-2-simple regex\n",
            "Installing collected packages: regex, toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.5.4 regex-2019.6.8 toposort-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1NV88fedT2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYblSBO0dtA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5f6b3b03-34d4-4f54-fb01-4bfa6ecf8f25"
      },
      "source": [
        "gpt2.download_gpt2()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 240Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 87.9Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 693Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 119Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 259Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 135Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 159Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SCf1lF0Irm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c772259d-8084-44ac-fa7b-69d7fadcbe46"
      },
      "source": [
        "enc = gpt2.encoder.get_encoder('models/117M/')\n",
        "data = gpt2.load_dataset(enc, path='./corpus', combine=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 406/406 [00:08<00:00, 46.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOZhx0cRK_Se",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5efbd072-31b1-43aa-8af2-5b67043dfec8"
      },
      "source": [
        "data[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3181,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Zp86O9dv0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dybxeM6cd77G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afeb0a34-5179-4f29-b24d-a99c37a4662c"
      },
      "source": [
        "# gpt2.load_gpt2(sess)\n",
        "gpt2.finetune(sess, './corpus/', steps=1000)\n",
        "gpt2.generate(sess)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 22:41:31.394274 139636705859456 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0807 22:41:46.253219 139636705859456 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/117M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 6/406 [00:00<00:09, 43.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 406/406 [00:09<00:00, 44.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1709206 tokens\n",
            "Training...\n",
            "[1 | 9.29] loss=3.02 avg=3.02\n",
            "[2 | 11.41] loss=2.36 avg=2.68\n",
            "[3 | 13.53] loss=2.71 avg=2.69\n",
            "[4 | 15.66] loss=2.29 avg=2.59\n",
            "[5 | 17.79] loss=2.62 avg=2.59\n",
            "[6 | 19.93] loss=2.31 avg=2.55\n",
            "[7 | 22.07] loss=2.39 avg=2.52\n",
            "[8 | 24.21] loss=2.69 avg=2.54\n",
            "[9 | 26.37] loss=2.89 avg=2.58\n",
            "[10 | 28.52] loss=2.63 avg=2.59\n",
            "[11 | 30.67] loss=2.61 avg=2.59\n",
            "[12 | 32.83] loss=2.44 avg=2.58\n",
            "[13 | 34.98] loss=2.39 avg=2.56\n",
            "[14 | 37.14] loss=2.48 avg=2.56\n",
            "[15 | 39.31] loss=2.43 avg=2.55\n",
            "[16 | 41.48] loss=2.66 avg=2.55\n",
            "[17 | 43.65] loss=2.68 avg=2.56\n",
            "[18 | 45.82] loss=2.29 avg=2.55\n",
            "[19 | 47.99] loss=2.22 avg=2.53\n",
            "[20 | 50.17] loss=2.47 avg=2.52\n",
            "[21 | 52.37] loss=2.42 avg=2.52\n",
            "[22 | 54.55] loss=2.49 avg=2.52\n",
            "[23 | 56.74] loss=2.66 avg=2.52\n",
            "[24 | 58.94] loss=2.77 avg=2.54\n",
            "[25 | 61.14] loss=2.40 avg=2.53\n",
            "[26 | 63.34] loss=2.31 avg=2.52\n",
            "[27 | 65.54] loss=2.16 avg=2.50\n",
            "[28 | 67.75] loss=2.44 avg=2.50\n",
            "[29 | 69.96] loss=2.63 avg=2.51\n",
            "[30 | 72.17] loss=2.30 avg=2.50\n",
            "[31 | 74.39] loss=2.14 avg=2.49\n",
            "[32 | 76.61] loss=2.42 avg=2.48\n",
            "[33 | 78.83] loss=2.26 avg=2.48\n",
            "[34 | 81.06] loss=2.49 avg=2.48\n",
            "[35 | 83.27] loss=2.42 avg=2.47\n",
            "[36 | 85.49] loss=2.22 avg=2.47\n",
            "[37 | 87.71] loss=2.12 avg=2.45\n",
            "[38 | 89.93] loss=2.15 avg=2.44\n",
            "[39 | 92.16] loss=2.46 avg=2.45\n",
            "[40 | 94.39] loss=2.37 avg=2.44\n",
            "[41 | 96.62] loss=2.43 avg=2.44\n",
            "[42 | 98.85] loss=2.44 avg=2.44\n",
            "[43 | 101.09] loss=2.36 avg=2.44\n",
            "[44 | 103.32] loss=2.38 avg=2.44\n",
            "[45 | 105.57] loss=2.59 avg=2.44\n",
            "[46 | 107.82] loss=2.33 avg=2.44\n",
            "[47 | 110.07] loss=2.48 avg=2.44\n",
            "[48 | 112.32] loss=2.63 avg=2.45\n",
            "[49 | 114.59] loss=2.31 avg=2.44\n",
            "[50 | 116.85] loss=2.48 avg=2.44\n",
            "[51 | 119.11] loss=2.55 avg=2.45\n",
            "[52 | 121.38] loss=2.25 avg=2.44\n",
            "[53 | 123.64] loss=2.51 avg=2.44\n",
            "[54 | 125.91] loss=2.26 avg=2.44\n",
            "[55 | 128.18] loss=2.71 avg=2.44\n",
            "[56 | 130.46] loss=2.25 avg=2.44\n",
            "[57 | 132.72] loss=2.09 avg=2.43\n",
            "[58 | 134.99] loss=2.45 avg=2.43\n",
            "[59 | 137.25] loss=2.28 avg=2.43\n",
            "[60 | 139.51] loss=2.34 avg=2.43\n",
            "[61 | 141.78] loss=2.38 avg=2.43\n",
            "[62 | 144.05] loss=2.35 avg=2.42\n",
            "[63 | 146.31] loss=2.35 avg=2.42\n",
            "[64 | 148.57] loss=2.30 avg=2.42\n",
            "[65 | 150.84] loss=2.29 avg=2.42\n",
            "[66 | 153.11] loss=2.53 avg=2.42\n",
            "[67 | 155.40] loss=2.10 avg=2.41\n",
            "[68 | 157.66] loss=2.31 avg=2.41\n",
            "[69 | 159.95] loss=2.35 avg=2.41\n",
            "[70 | 162.23] loss=2.41 avg=2.41\n",
            "[71 | 164.51] loss=2.46 avg=2.41\n",
            "[72 | 166.80] loss=2.40 avg=2.41\n",
            "[73 | 169.08] loss=2.39 avg=2.41\n",
            "[74 | 171.37] loss=2.52 avg=2.41\n",
            "[75 | 173.66] loss=2.11 avg=2.41\n",
            "[76 | 175.95] loss=2.13 avg=2.40\n",
            "[77 | 178.24] loss=2.30 avg=2.40\n",
            "[78 | 180.53] loss=2.32 avg=2.40\n",
            "[79 | 182.83] loss=2.62 avg=2.40\n",
            "[80 | 185.13] loss=2.33 avg=2.40\n",
            "[81 | 187.42] loss=2.15 avg=2.40\n",
            "[82 | 189.72] loss=2.17 avg=2.39\n",
            "[83 | 192.02] loss=1.89 avg=2.38\n",
            "[84 | 194.33] loss=2.13 avg=2.38\n",
            "[85 | 196.63] loss=2.15 avg=2.38\n",
            "[86 | 198.94] loss=2.22 avg=2.37\n",
            "[87 | 201.25] loss=2.44 avg=2.37\n",
            "[88 | 203.56] loss=2.22 avg=2.37\n",
            "[89 | 205.88] loss=2.38 avg=2.37\n",
            "[90 | 208.20] loss=2.12 avg=2.37\n",
            "[91 | 210.51] loss=2.28 avg=2.37\n",
            "[92 | 212.82] loss=2.29 avg=2.36\n",
            "[93 | 215.15] loss=2.41 avg=2.37\n",
            "[94 | 217.46] loss=2.63 avg=2.37\n",
            "[95 | 219.79] loss=2.38 avg=2.37\n",
            "[96 | 222.11] loss=2.25 avg=2.37\n",
            "[97 | 224.43] loss=2.17 avg=2.36\n",
            "[98 | 226.77] loss=2.57 avg=2.37\n",
            "[99 | 229.09] loss=2.20 avg=2.36\n",
            "[100 | 231.43] loss=2.26 avg=2.36\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "The \"Newest\" column is the oldest line with a timestamp, the other column the entry is currently on (if there is a valid timestamp in a\n",
            "subsection), and the \"Newest\" column is the newest line from this\n",
            "archive, if there is a valid timestamp on the entry.\n",
            "The \"Newest\" column is the oldest line from thisarchive, the other\n",
            "column the entry currently on (if there is a valid timestamp in a\n",
            "subsection), and the \"Newest\" column is the current entry in the\n",
            "archive (if there is at least one valid entry in the\n",
            "archive).\n",
            "\n",
            "\n",
            "\n",
            "Changed in version 3.7: Added new syntax for \"Entry of Newest and Previously\" column.\n",
            "The \"Newest\" column is the oldest line with a timestamp and other lines of entry with\n",
            "the timestamp.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.7.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.4.5.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New: Newest table\n",
            "Newest\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New: Newest\n",
            "Newest - oldest\n",
            "\n",
            "(Last modified: Wed Dec 9, 2004 10:56:56 EDT)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.7.\n",
            "\n",
            "\n",
            "\n",
            "Changed in version 3.6: Show and Hide items now display the timestamp and\n",
            "the current entry from the archive\n",
            "only if the archive has a valid timestamp.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New: Added new line.\n",
            "Changed in version 3.6: Show and Hide items now display the timestamp and\n",
            "the current entry from the archive only if the archive has a valid timestamp.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New: Fixed a bug when deleting items in the \"Newest\" column.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.4: All new formatters added, using \n",
            "the new format() method.\n",
            "See also File:Formatters.\n",
            "\n",
            "\n",
            "Changed in version 3.3: Added the use of \"Last updated on: 12/4/10 09:13:18\" to the\n",
            "list of files to be checked in the database.\n",
            "\n",
            "\n",
            "New in version 3.1.\n",
            "New:\n",
            "New: Added the use of Formatters.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 7.7: Updated the                    section\n",
            "of the archive to correct the error raised by the -n flag to the last line to the\n",
            "current entry in the archives.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "new in version 3.7: Showing and Hide items (now only show only the current entry) now has a\n",
            "statistic attribute to display.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.5.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.5.\n",
            "\n",
            "\n",
            "\n",
            "Changed in version 3.3.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.1: Added a new Formatter.                \n",
            "                                 \n",
            "                    New\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.5: Added a new Formatter, showing the current entry (if a valid\n",
            "entry in the archive exists) and the entry and a string if it is missing\n",
            "“last available”.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changed in version 3.4: New information for the archive entries is now provided.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.0.\n",
            "New:       \n",
            "       Report a Report an error.     \n",
            "       Show a report for a specific entry.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changed in version 3.3: Added the use of Formatters and the Report a Report a Report an Error as the Formatters.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.1: Changed the type of the Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report a Report\n",
            "\n",
            "[101 | 245.68] loss=2.43 avg=2.36\n",
            "[102 | 248.02] loss=2.21 avg=2.36\n",
            "[103 | 250.35] loss=2.22 avg=2.36\n",
            "[104 | 252.69] loss=2.04 avg=2.35\n",
            "[105 | 255.03] loss=2.63 avg=2.36\n",
            "[106 | 257.37] loss=2.33 avg=2.36\n",
            "[107 | 259.72] loss=2.19 avg=2.36\n",
            "[108 | 262.06] loss=2.30 avg=2.36\n",
            "[109 | 264.40] loss=1.90 avg=2.35\n",
            "[110 | 266.74] loss=2.29 avg=2.35\n",
            "[111 | 269.09] loss=2.15 avg=2.34\n",
            "[112 | 271.43] loss=2.08 avg=2.34\n",
            "[113 | 273.77] loss=2.22 avg=2.34\n",
            "[114 | 276.12] loss=2.78 avg=2.35\n",
            "[115 | 278.47] loss=2.02 avg=2.34\n",
            "[116 | 280.82] loss=2.56 avg=2.34\n",
            "[117 | 283.16] loss=2.16 avg=2.34\n",
            "[118 | 285.52] loss=2.24 avg=2.34\n",
            "[119 | 287.87] loss=2.15 avg=2.34\n",
            "[120 | 290.23] loss=2.28 avg=2.34\n",
            "[121 | 292.58] loss=1.92 avg=2.33\n",
            "[122 | 294.94] loss=2.07 avg=2.33\n",
            "[123 | 297.31] loss=2.14 avg=2.32\n",
            "[124 | 299.66] loss=2.09 avg=2.32\n",
            "[125 | 302.01] loss=2.46 avg=2.32\n",
            "[126 | 304.37] loss=2.62 avg=2.33\n",
            "[127 | 306.71] loss=2.38 avg=2.33\n",
            "[128 | 309.07] loss=2.40 avg=2.33\n",
            "[129 | 311.43] loss=2.02 avg=2.32\n",
            "[130 | 313.78] loss=2.32 avg=2.32\n",
            "[131 | 316.14] loss=2.54 avg=2.33\n",
            "[132 | 318.50] loss=1.94 avg=2.32\n",
            "[133 | 320.88] loss=2.25 avg=2.32\n",
            "[134 | 323.23] loss=2.28 avg=2.32\n",
            "[135 | 325.61] loss=2.12 avg=2.32\n",
            "[136 | 327.97] loss=2.24 avg=2.32\n",
            "[137 | 330.34] loss=2.08 avg=2.31\n",
            "[138 | 332.70] loss=2.54 avg=2.32\n",
            "[139 | 335.06] loss=2.42 avg=2.32\n",
            "[140 | 337.42] loss=2.13 avg=2.32\n",
            "[141 | 339.79] loss=1.92 avg=2.31\n",
            "[142 | 342.15] loss=2.24 avg=2.31\n",
            "[143 | 344.52] loss=2.49 avg=2.31\n",
            "[144 | 346.88] loss=2.28 avg=2.31\n",
            "[145 | 349.24] loss=2.29 avg=2.31\n",
            "[146 | 351.60] loss=2.28 avg=2.31\n",
            "[147 | 353.97] loss=1.96 avg=2.31\n",
            "[148 | 356.33] loss=2.35 avg=2.31\n",
            "[149 | 358.69] loss=2.12 avg=2.30\n",
            "[150 | 361.07] loss=2.20 avg=2.30\n",
            "[151 | 363.43] loss=2.19 avg=2.30\n",
            "[152 | 365.79] loss=2.11 avg=2.30\n",
            "[153 | 368.17] loss=2.11 avg=2.30\n",
            "[154 | 370.53] loss=2.34 avg=2.30\n",
            "[155 | 372.90] loss=2.50 avg=2.30\n",
            "[156 | 375.27] loss=2.33 avg=2.30\n",
            "[157 | 377.64] loss=1.85 avg=2.29\n",
            "[158 | 380.01] loss=2.28 avg=2.29\n",
            "[159 | 382.37] loss=2.47 avg=2.30\n",
            "[160 | 384.74] loss=1.89 avg=2.29\n",
            "[161 | 387.10] loss=2.14 avg=2.29\n",
            "[162 | 389.47] loss=2.40 avg=2.29\n",
            "[163 | 391.84] loss=2.20 avg=2.29\n",
            "[164 | 394.20] loss=2.06 avg=2.29\n",
            "[165 | 396.58] loss=2.08 avg=2.28\n",
            "[166 | 398.95] loss=2.68 avg=2.29\n",
            "[167 | 401.32] loss=2.43 avg=2.29\n",
            "[168 | 403.69] loss=2.30 avg=2.29\n",
            "[169 | 406.06] loss=2.26 avg=2.29\n",
            "[170 | 408.43] loss=2.03 avg=2.29\n",
            "[171 | 410.80] loss=2.37 avg=2.29\n",
            "[172 | 413.17] loss=2.37 avg=2.29\n",
            "[173 | 415.55] loss=2.12 avg=2.29\n",
            "[174 | 417.92] loss=2.46 avg=2.29\n",
            "[175 | 420.28] loss=2.22 avg=2.29\n",
            "[176 | 422.67] loss=2.26 avg=2.29\n",
            "[177 | 425.03] loss=1.94 avg=2.28\n",
            "[178 | 427.41] loss=2.19 avg=2.28\n",
            "[179 | 429.77] loss=2.01 avg=2.28\n",
            "[180 | 432.14] loss=2.09 avg=2.28\n",
            "[181 | 434.52] loss=2.43 avg=2.28\n",
            "[182 | 436.89] loss=1.97 avg=2.28\n",
            "[183 | 439.26] loss=2.26 avg=2.28\n",
            "[184 | 441.64] loss=2.34 avg=2.28\n",
            "[185 | 444.01] loss=2.48 avg=2.28\n",
            "[186 | 446.38] loss=2.21 avg=2.28\n",
            "[187 | 448.75] loss=2.22 avg=2.28\n",
            "[188 | 451.12] loss=2.39 avg=2.28\n",
            "[189 | 453.49] loss=2.15 avg=2.28\n",
            "[190 | 455.87] loss=2.33 avg=2.28\n",
            "[191 | 458.25] loss=2.38 avg=2.28\n",
            "[192 | 460.62] loss=2.26 avg=2.28\n",
            "[193 | 463.00] loss=2.50 avg=2.28\n",
            "[194 | 465.38] loss=2.38 avg=2.28\n",
            "[195 | 467.75] loss=2.19 avg=2.28\n",
            "[196 | 470.12] loss=2.39 avg=2.28\n",
            "[197 | 472.50] loss=2.21 avg=2.28\n",
            "[198 | 474.87] loss=2.25 avg=2.28\n",
            "[199 | 477.24] loss=2.34 avg=2.28\n",
            "[200 | 479.63] loss=2.03 avg=2.28\n",
            "======== SAMPLE 1 ========\n",
            ">>>\n",
            "\n",
            "     \n",
            "1       \n",
            "  Previous topic\n",
            "        Showing Custom Functionality\n",
            "  Next topic\n",
            "       Report a Bug\n",
            "     \n",
            "       Showing the difference between Python’s constructor and function objects\n",
            "      \n",
            "        Showing Python’s Type System\n",
            "     \n",
            "        This Page\n",
            "     \n",
            "      Report a Bug\n",
            "     \n",
            "        Showing Python’s Type System\n",
            "      \n",
            "        This Page\n",
            "     \n",
            "      \n",
            "       Report a Bug\n",
            "     \n",
            "\n",
            "   Previous topic\n",
            "   Python’s Type System\n",
            "  Next topic  Python’s Type System\n",
            "    \n",
            "     This Page\n",
            "    \n",
            "      Report a Bug\n",
            "      \n",
            "       Showing Python’s Type System\n",
            "   Previous topic\n",
            "   This Page\n",
            "   \n",
            "     Report a Bug\n",
            "     \n",
            "        Report a Bug\n",
            "          Showing Python’s Type System\n",
            "  Previous topic<|endoftext|>\"I'm a single, single man, I’m your man.\"\n",
            "The man who calls \"you a man, I'm your man\" may have a name, but not a real name. The name is the phrase you use to describe yourself or your people, and it is not necessarily a good one. It, too, is subject to changing.\n",
            "For an advanced look at the meaning and history of the phrase \"I'm a single, single man, I'm your man,\" see Why I'm a Man, Why I'm a Single.\n",
            "1. It is to indicate a state that you are a man; it is only a man's name that counts\n",
            "in\n",
            "1. The phrase implies a state of being your\n",
            "one and only man.   This is a formalized definition of\n",
            "the phrase.\n",
            "\"I'm a man, I'm your man.\" The standard usage for this is:\n",
            "\n",
            "A man is a man or a man-for-hisself, or a man for-selves\n",
            "or not their man. There is no such thing as a man.\n",
            "\n",
            "1.1.1. The phrase implies a condition that man is\n",
            "something that he is, whether he is-self,\n",
            "self-for-self or not, and is not a condition that man has\n",
            "something to do with, as well as how.  The\n",
            "example used here is to help illustrate the distinction between different\n",
            "types of man. In this definition,\n",
            "(name) indicates that a man should be a man, a man-for-self or not\n",
            "self-for-self, but not in that order.  It does not mean that\n",
            "man is a man; it only means that a man is\n",
            "something that he is, whether he is-self,\n",
            "self-for-self or not, and is not a condition that man has\n",
            "something not to do with, as well as how.  It does not\n",
            "mean that a man is a man; it only means that a man has\n",
            "something to do with, as well as how.  It does not mean that\n",
            "man has something to do with, it only means that a man has\n",
            "something to do with, as well as how.  It does not mean that\n",
            "man has something to do with, it only means that a man has\n",
            "something to do with, as well as how.  It does not mean that\n",
            "man has something to do with, it only means that a man has\n",
            "something to do with, as well as how.  It does not mean that a\n",
            "man has something to do with, it only means that a man has\n",
            "something to do with, as well as how.  It does not mean that a\n",
            "man knows that he has something to do with, it only\n",
            "means that (name) is a man, if there is a\n",
            "man (name).  It also means that (name)\n",
            "is a condition that (name).  It also means that (name) has a\n",
            "condition that (name).  It also means that (name).  It does not\n",
            "mean that a man is a man; it does not mean that a man has\n",
            "something to do with, it only means that (name).  It does not\n",
            "mean that a man has anything to do with, it only means that a\n",
            "\n",
            "\n",
            "[201 | 492.74] loss=1.97 avg=2.28\n",
            "[202 | 495.12] loss=2.36 avg=2.28\n",
            "[203 | 497.49] loss=1.86 avg=2.27\n",
            "[204 | 499.86] loss=2.06 avg=2.27\n",
            "[205 | 502.24] loss=1.89 avg=2.26\n",
            "[206 | 504.61] loss=2.45 avg=2.27\n",
            "[207 | 507.00] loss=2.08 avg=2.26\n",
            "[208 | 509.39] loss=2.19 avg=2.26\n",
            "[209 | 511.77] loss=2.13 avg=2.26\n",
            "[210 | 514.15] loss=1.93 avg=2.26\n",
            "[211 | 516.53] loss=2.07 avg=2.26\n",
            "[212 | 518.90] loss=2.45 avg=2.26\n",
            "[213 | 521.27] loss=2.44 avg=2.26\n",
            "[214 | 523.66] loss=2.14 avg=2.26\n",
            "[215 | 526.04] loss=2.23 avg=2.26\n",
            "[216 | 528.42] loss=2.05 avg=2.26\n",
            "[217 | 530.80] loss=2.25 avg=2.26\n",
            "[218 | 533.19] loss=2.14 avg=2.26\n",
            "[219 | 535.57] loss=1.90 avg=2.25\n",
            "[220 | 537.95] loss=2.18 avg=2.25\n",
            "[221 | 540.34] loss=2.07 avg=2.25\n",
            "[222 | 542.72] loss=2.22 avg=2.25\n",
            "[223 | 545.09] loss=1.97 avg=2.25\n",
            "[224 | 547.48] loss=2.01 avg=2.24\n",
            "[225 | 549.87] loss=2.37 avg=2.24\n",
            "[226 | 552.25] loss=2.25 avg=2.24\n",
            "[227 | 554.63] loss=2.16 avg=2.24\n",
            "[228 | 557.02] loss=1.96 avg=2.24\n",
            "[229 | 559.40] loss=2.24 avg=2.24\n",
            "[230 | 561.78] loss=2.24 avg=2.24\n",
            "[231 | 564.17] loss=1.89 avg=2.24\n",
            "[232 | 566.55] loss=2.26 avg=2.24\n",
            "[233 | 568.93] loss=1.96 avg=2.23\n",
            "[234 | 571.31] loss=2.00 avg=2.23\n",
            "[235 | 573.69] loss=2.23 avg=2.23\n",
            "[236 | 576.07] loss=2.46 avg=2.23\n",
            "[237 | 578.46] loss=2.17 avg=2.23\n",
            "[238 | 580.85] loss=2.19 avg=2.23\n",
            "[239 | 583.22] loss=2.23 avg=2.23\n",
            "[240 | 585.62] loss=2.17 avg=2.23\n",
            "[241 | 588.00] loss=2.09 avg=2.23\n",
            "[242 | 590.37] loss=1.77 avg=2.22\n",
            "[243 | 592.76] loss=2.27 avg=2.23\n",
            "[244 | 595.13] loss=2.06 avg=2.22\n",
            "[245 | 597.51] loss=2.26 avg=2.22\n",
            "[246 | 599.91] loss=2.06 avg=2.22\n",
            "[247 | 602.29] loss=1.94 avg=2.22\n",
            "[248 | 604.67] loss=2.04 avg=2.22\n",
            "[249 | 607.06] loss=1.92 avg=2.21\n",
            "[250 | 609.44] loss=2.25 avg=2.21\n",
            "[251 | 611.83] loss=2.15 avg=2.21\n",
            "[252 | 614.21] loss=2.04 avg=2.21\n",
            "[253 | 616.58] loss=2.11 avg=2.21\n",
            "[254 | 618.98] loss=2.18 avg=2.21\n",
            "[255 | 621.35] loss=2.11 avg=2.21\n",
            "[256 | 623.75] loss=2.18 avg=2.21\n",
            "[257 | 626.14] loss=2.02 avg=2.21\n",
            "[258 | 628.52] loss=2.21 avg=2.21\n",
            "[259 | 630.90] loss=2.10 avg=2.21\n",
            "[260 | 633.29] loss=2.43 avg=2.21\n",
            "[261 | 635.67] loss=2.09 avg=2.21\n",
            "[262 | 638.05] loss=2.10 avg=2.21\n",
            "[263 | 640.45] loss=2.39 avg=2.21\n",
            "[264 | 642.83] loss=2.31 avg=2.21\n",
            "[265 | 645.21] loss=2.35 avg=2.21\n",
            "[266 | 647.60] loss=2.12 avg=2.21\n",
            "[267 | 649.98] loss=2.25 avg=2.21\n",
            "[268 | 652.37] loss=2.15 avg=2.21\n",
            "[269 | 654.75] loss=2.30 avg=2.21\n",
            "[270 | 657.14] loss=1.87 avg=2.21\n",
            "[271 | 659.52] loss=2.31 avg=2.21\n",
            "[272 | 661.91] loss=1.93 avg=2.20\n",
            "[273 | 664.29] loss=2.00 avg=2.20\n",
            "[274 | 666.68] loss=1.64 avg=2.20\n",
            "[275 | 669.06] loss=2.07 avg=2.19\n",
            "[276 | 671.45] loss=2.05 avg=2.19\n",
            "[277 | 673.83] loss=2.15 avg=2.19\n",
            "[278 | 676.20] loss=2.11 avg=2.19\n",
            "[279 | 678.59] loss=2.17 avg=2.19\n",
            "[280 | 680.97] loss=2.17 avg=2.19\n",
            "[281 | 683.34] loss=2.08 avg=2.19\n",
            "[282 | 685.72] loss=2.21 avg=2.19\n",
            "[283 | 688.12] loss=1.95 avg=2.19\n",
            "[284 | 690.50] loss=2.14 avg=2.19\n",
            "[285 | 692.88] loss=2.13 avg=2.19\n",
            "[286 | 695.27] loss=2.47 avg=2.19\n",
            "[287 | 697.66] loss=2.27 avg=2.19\n",
            "[288 | 700.05] loss=2.02 avg=2.19\n",
            "[289 | 702.42] loss=2.09 avg=2.19\n",
            "[290 | 704.80] loss=2.04 avg=2.19\n",
            "[291 | 707.18] loss=2.23 avg=2.19\n",
            "[292 | 709.57] loss=2.33 avg=2.19\n",
            "[293 | 711.95] loss=2.14 avg=2.19\n",
            "[294 | 714.35] loss=2.24 avg=2.19\n",
            "[295 | 716.73] loss=1.90 avg=2.19\n",
            "[296 | 719.12] loss=1.76 avg=2.18\n",
            "[297 | 721.50] loss=2.19 avg=2.18\n",
            "[298 | 723.88] loss=1.97 avg=2.18\n",
            "[299 | 726.27] loss=2.14 avg=2.18\n",
            "[300 | 728.65] loss=1.90 avg=2.18\n",
            "======== SAMPLE 1 ========\n",
            " reference (if defined) by using the context managers from C to run\n",
            "some function. This is normally done implicitly by the Python built-in context managers, for instance by setting\n",
            "the __context__ attribute on a C context manager. For use with Python, it might also be\n",
            "needed to have another context manager call that function.\n",
            "\n",
            "Changed in version 3.2: Added the getcontext_variables() and getcontext_object() methods.\n",
            "\n",
            "\n",
            "Changed in version 3.5: The context manager for the Python built-in context manager is now a subclass of the\n",
            "context managers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contextmanagement.getprec_pre()\n",
            "Return a list of all calls to the context management function with arguments.  If\n",
            "not defined or unrooted, this is the function which sets up the instance for\n",
            "the Python built-in context manager.  You can read the documentation of any\n",
            "call to this function by calling it with the context manager as the argument.\n",
            ">>> contextmanagement.getprec_pre(\"example:foo\")\n",
            "<Example:foo, argc=(2), context=None>\n",
            "\n",
            "Note that the instance was built into the context manager when the context manager was\n",
            "set up with the function.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contextmanagement.getprec_object()\n",
            "Return an instance object of the context management function. If no\n",
            "return value is defined, this is the function which set up the instance for\n",
            "the Python built-in context manager.  If the instance instance is a pointer to some\n",
            "object, it must have been built into the context manager and has been initialized with\n",
            "the pointer as well.\n",
            ">>> contextmanagement.getprec_object(\"defaults\")\n",
            "<defaults, argc=(0, 0, 1)>\n",
            "\n",
            "\n",
            "The default value argument is used when the Python built-in context manager does not use any\n",
            "context management functions.\n",
            ">>> contextmanagement.getprec_object(\"defaults\")\n",
            "<defaults, argc=(0, 0, 1)>\n",
            "\n",
            "The default keyword argument is used when the Python built-in context manager does not use\n",
            "Context Manager classes as an argument.  The return value of\n",
            "contextmanagement.getprec_object() is NULL.\n",
            "The returned object usually takes only the value of its Context Manager\n",
            "instance that is defined to have a default value. When the context manager\n",
            "sources its default instance (or None), the default instance argument defaults to\n",
            "NULL, instead of NULL.\n",
            "\n",
            "\n",
            "Note that not all context managers use this function; if only a subset are used,\n",
            "they should return NULL. Also note that the default instance returned by\n",
            "getprec_object() implicitly defaults to NULL: if your context manager uses\n",
            "getprec_object() instead of context, a NULL value will be returned from that\n",
            "call.\n",
            "\n",
            "\n",
            "The context management function implicitly sets up its own instance, using the\n",
            "instance to which it was bound. This is necessary because all Python\n",
            "context managers implicitly set the default instance to NULL for all C\n",
            "context managers except those named with the context manager constructor.\n",
            "Since the default instance is NULL, the class default() is implicitly cast to\n",
            "NULL using the context manager.\n",
            "\n",
            "\n",
            "\n",
            "Note\n",
            "It is common to use the __context__ attribute of instance() instances with\n",
            "the context manager in the context manager for the instance it is bound to. In this case,\n",
            "this method will not cast the default instance to NULL.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contextmanagement.setprec_instance(context)\n",
            "The function setprec_object(context).\n",
            "In Python, context will be set to the default instance of the underlying context\n",
            "managers instance. This is a convenience function to prevent unnecessary\n",
            "instances being created by using setprec_pre() on a subclass of the\n",
            "context manager interface.\n",
            "Because this function does nothing useful during the execution of a function, it is not\n",
            "a good idea to use it during compilation, especially when you are using\n",
            "the context manager with the Python built-in context manager.\n",
            "When using this function in context-specific code, keep in mind that if the\n",
            "method is named with context, the instance is not initialized to\n",
            "otherwise have context manager instances set to NULL for that class.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contextmanagement.setprec_object(context)\n",
            "The function setprec_object(context) provides the default object used\n",
            "in the context manager for a class-level context manager.\n",
            "The function sets up its own instance of context management.  This is necessary because\n",
            "all Python context managers implicitly set the default instance to NULL for all C\n",
            "context managers except those named with the context manager constructor.\n",
            "Since the default instance is created by setting context_variables() and\n",
            "getcontext_object() to NULL before setting the default instance to NULL,\n",
            "this method will not set default instances to NULL on any C and other contexts\n",
            "using the\n",
            "\n",
            "[301 | 741.73] loss=2.00 avg=2.17\n",
            "[302 | 744.10] loss=2.09 avg=2.17\n",
            "[303 | 746.50] loss=2.08 avg=2.17\n",
            "[304 | 748.89] loss=2.07 avg=2.17\n",
            "[305 | 751.29] loss=1.95 avg=2.17\n",
            "[306 | 753.68] loss=2.02 avg=2.17\n",
            "[307 | 756.08] loss=2.00 avg=2.16\n",
            "[308 | 758.47] loss=2.06 avg=2.16\n",
            "[309 | 760.85] loss=2.28 avg=2.16\n",
            "[310 | 763.25] loss=2.20 avg=2.17\n",
            "[311 | 765.63] loss=2.05 avg=2.16\n",
            "[312 | 768.02] loss=2.15 avg=2.16\n",
            "[313 | 770.40] loss=2.37 avg=2.17\n",
            "[314 | 772.79] loss=1.95 avg=2.16\n",
            "[315 | 775.17] loss=1.72 avg=2.16\n",
            "[316 | 777.55] loss=2.05 avg=2.16\n",
            "[317 | 779.94] loss=2.03 avg=2.16\n",
            "[318 | 782.33] loss=2.39 avg=2.16\n",
            "[319 | 784.72] loss=2.08 avg=2.16\n",
            "[320 | 787.12] loss=1.97 avg=2.16\n",
            "[321 | 789.51] loss=2.07 avg=2.16\n",
            "[322 | 791.89] loss=2.07 avg=2.15\n",
            "[323 | 794.28] loss=2.23 avg=2.16\n",
            "[324 | 796.67] loss=2.17 avg=2.16\n",
            "[325 | 799.07] loss=1.75 avg=2.15\n",
            "[326 | 801.47] loss=2.16 avg=2.15\n",
            "[327 | 803.85] loss=2.11 avg=2.15\n",
            "[328 | 806.24] loss=2.19 avg=2.15\n",
            "[329 | 808.62] loss=1.90 avg=2.15\n",
            "[330 | 811.00] loss=2.03 avg=2.15\n",
            "[331 | 813.40] loss=2.04 avg=2.15\n",
            "[332 | 815.78] loss=1.88 avg=2.14\n",
            "[333 | 818.16] loss=1.91 avg=2.14\n",
            "[334 | 820.56] loss=2.05 avg=2.14\n",
            "[335 | 822.94] loss=2.17 avg=2.14\n",
            "[336 | 825.33] loss=2.07 avg=2.14\n",
            "[337 | 827.72] loss=2.28 avg=2.14\n",
            "[338 | 830.10] loss=2.09 avg=2.14\n",
            "[339 | 832.50] loss=1.99 avg=2.14\n",
            "[340 | 834.87] loss=1.78 avg=2.14\n",
            "[341 | 837.26] loss=2.15 avg=2.14\n",
            "[342 | 839.65] loss=2.13 avg=2.14\n",
            "[343 | 842.03] loss=2.03 avg=2.13\n",
            "[344 | 844.41] loss=2.26 avg=2.14\n",
            "[345 | 846.79] loss=1.66 avg=2.13\n",
            "[346 | 849.19] loss=2.39 avg=2.13\n",
            "[347 | 851.58] loss=1.91 avg=2.13\n",
            "[348 | 853.96] loss=1.58 avg=2.13\n",
            "[349 | 856.35] loss=1.97 avg=2.12\n",
            "[350 | 858.74] loss=1.85 avg=2.12\n",
            "[351 | 861.14] loss=2.26 avg=2.12\n",
            "[352 | 863.53] loss=1.67 avg=2.12\n",
            "[353 | 865.91] loss=2.11 avg=2.12\n",
            "[354 | 868.30] loss=1.96 avg=2.12\n",
            "[355 | 870.70] loss=2.09 avg=2.12\n",
            "[356 | 873.09] loss=2.36 avg=2.12\n",
            "[357 | 875.48] loss=2.15 avg=2.12\n",
            "[358 | 877.86] loss=1.90 avg=2.12\n",
            "[359 | 880.24] loss=1.66 avg=2.11\n",
            "[360 | 882.64] loss=2.30 avg=2.11\n",
            "[361 | 885.03] loss=2.16 avg=2.11\n",
            "[362 | 887.42] loss=2.28 avg=2.12\n",
            "[363 | 889.80] loss=1.94 avg=2.11\n",
            "[364 | 892.20] loss=2.14 avg=2.11\n",
            "[365 | 894.59] loss=2.02 avg=2.11\n",
            "[366 | 896.98] loss=2.23 avg=2.11\n",
            "[367 | 899.37] loss=1.67 avg=2.11\n",
            "[368 | 901.76] loss=2.21 avg=2.11\n",
            "[369 | 904.15] loss=2.06 avg=2.11\n",
            "[370 | 906.53] loss=2.21 avg=2.11\n",
            "[371 | 908.92] loss=1.94 avg=2.11\n",
            "[372 | 911.30] loss=2.45 avg=2.11\n",
            "[373 | 913.70] loss=2.14 avg=2.11\n",
            "[374 | 916.08] loss=1.82 avg=2.11\n",
            "[375 | 918.47] loss=1.94 avg=2.11\n",
            "[376 | 920.85] loss=2.20 avg=2.11\n",
            "[377 | 923.24] loss=1.99 avg=2.11\n",
            "[378 | 925.63] loss=2.25 avg=2.11\n",
            "[379 | 928.02] loss=1.83 avg=2.11\n",
            "[380 | 930.42] loss=1.96 avg=2.11\n",
            "[381 | 932.81] loss=1.95 avg=2.10\n",
            "[382 | 935.20] loss=1.80 avg=2.10\n",
            "[383 | 937.59] loss=2.31 avg=2.10\n",
            "[384 | 939.97] loss=2.07 avg=2.10\n",
            "[385 | 942.36] loss=2.35 avg=2.11\n",
            "[386 | 944.74] loss=2.18 avg=2.11\n",
            "[387 | 947.13] loss=1.84 avg=2.10\n",
            "[388 | 949.53] loss=2.14 avg=2.10\n",
            "[389 | 951.92] loss=2.03 avg=2.10\n",
            "[390 | 954.31] loss=2.22 avg=2.10\n",
            "[391 | 956.69] loss=2.19 avg=2.11\n",
            "[392 | 959.07] loss=1.97 avg=2.10\n",
            "[393 | 961.45] loss=1.93 avg=2.10\n",
            "[394 | 963.84] loss=2.00 avg=2.10\n",
            "[395 | 966.23] loss=2.01 avg=2.10\n",
            "[396 | 968.62] loss=2.15 avg=2.10\n",
            "[397 | 971.01] loss=2.10 avg=2.10\n",
            "[398 | 973.40] loss=1.95 avg=2.10\n",
            "[399 | 975.77] loss=1.97 avg=2.10\n",
            "[400 | 978.16] loss=1.78 avg=2.09\n",
            "======== SAMPLE 1 ========\n",
            "           # print, %s: [10, 9, 6, 3, 1, 0]\n",
            "\n",
            "\n",
            "print(stdout.gettext())\n",
            "Print output\n",
            "Printing version information...\n",
            "\n",
            "Warning: If this function returns no, __getstat__() should not be used.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          \n",
            "         \n",
            "       \n",
            "         \n",
            "   Table of Contents\n",
            "  Previous topic\n",
            "  File Import — Import and export files\n",
            "  Next topic\n",
            "  System Services — Services to manage files or other structured data\n",
            "\n",
            "   This Page\n",
            "   \n",
            "     Report a Bug\n",
            "     \n",
            "       Show Source<|endoftext|>1. Examples\n",
            "The examples in this chapter show how to run C code that runs programmatically in shell.\n",
            "This chapter is about various examples that allow working with C++ programs or shell code.\n",
            "The example examples are not meant, however, to be interpreted as command line strings;\n",
            "though, it could look a bit like this:\n",
            "/*\n",
            "Hello world!\n",
            "/*\n",
            "hello\n",
            "/*\n",
            "\n",
            "\n",
            "*/\n",
            "\\t.     /*:\n",
            "Hello world\n",
            "/*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  Previous topic\n",
            "  examples — Examples for use in ShellExec.Shell\n",
            "  Next topic\n",
            "  Examples — ShellExec.Shell\n",
            "  \n",
            "    This Page\n",
            "    \n",
            "      Report a Bug\n",
            "      \n",
            "        Show Source<|endoftext|>3. The shell debugger\n",
            "This chapter presents some examples showing how to use a shell debugger in a shell debugger.\n",
            "The examples are meant to be the base for more complete examples of executing shell functions.\n",
            "\n",
            "\n",
            "3.1. Writing and executing shell commands\n",
            "The following sections describe the various commands executed in the shell, the execution order of shells\n",
            "and the use cases of each command. The section on execution order is equivalent to:\n",
            "\n",
            "\\t.     \"d\" executes\n",
            "\n",
            ":d.     \"b\" executes\n",
            "\n",
            "D.     \"m\" executes\n",
            "\n",
            "D.     \"v\" executes\n",
            "\n",
            "\"M\" executes\n",
            "\n",
            "V.     \"b\" executes\n",
            "\n",
            "M.     \"m\" executes\n",
            "\n",
            "\"M.b\" executes\n",
            "\n",
            "The two sections have different uses for shell variables and command\n",
            "instances. The purpose of this chapter is to describe the use of shell variables for command and\n",
            "instances without making assumptions about the value of the variables in the\n",
            "argument or with respect to executing them.\n",
            "\n",
            "\n",
            "3.2. Writing and executing shell commands in the interpreter\n",
            "Writing shell commands to a program is not as straightforward as writing shell shell code.\n",
            "It is very possible that you will write code that executes a shell-based application like a web browser.  Therefore, the solution\n",
            "follows.\n",
            "You should write a simple C-style script which executes one standard C macro, and\n",
            "the macro may be overridden using standard C functions. There is also the possibility of overraring\n",
            "the macro with an external function, which is called with the name of the function to be overridden.\n",
            "There are two general ways to write a script: write (with C++ type overrulation); and\n",
            "executor.\n",
            "Writing custom code using C++ type overrulation in the interpreter may be very useful for those\n",
            "who want to write shell-based user code, or even user-defined application code.\n",
            "Writing your own script is usually very simple, and the process of writing is almost always\n",
            "considered a task for the reader as compared to the task being run by the script itself:\n",
            "Write a script.\n",
            "The reader needs to know what parameters are given through get(char_name) and\n",
            "get(str_name).  To provide context, get the corresponding\n",
            "parameter name (or null, depending on the shell environment), then execute\n",
            "the invocation of the shell function and return a file object with the given name, with a\n",
            "line number and a name which matches the supplied string.\n",
            "A similar form, written like this:\n",
            "...    try:\n",
            "       getname = char_name(\n",
            "       name);\n",
            "...except as mentioned above, will be called at\n",
            "...      __execute(sys.argv[0]);\n",
            "...except that this would be called at line number 0, rather than executing it\n",
            "...    and returning a byte string, which is used in the shell\n",
            "execution order, and then to indicate the execution order.\n",
            "...\n",
            "Some users may be\n",
            "\n",
            "[401 | 991.26] loss=2.23 avg=2.10\n",
            "[402 | 993.65] loss=2.21 avg=2.10\n",
            "[403 | 996.04] loss=1.94 avg=2.10\n",
            "[404 | 998.43] loss=1.68 avg=2.09\n",
            "[405 | 1000.81] loss=1.82 avg=2.09\n",
            "[406 | 1003.19] loss=1.87 avg=2.09\n",
            "[407 | 1005.59] loss=1.84 avg=2.08\n",
            "[408 | 1007.98] loss=1.93 avg=2.08\n",
            "[409 | 1010.37] loss=2.21 avg=2.08\n",
            "[410 | 1012.77] loss=2.23 avg=2.08\n",
            "[411 | 1015.16] loss=2.21 avg=2.09\n",
            "[412 | 1017.55] loss=1.94 avg=2.08\n",
            "[413 | 1019.96] loss=2.09 avg=2.08\n",
            "[414 | 1022.34] loss=2.07 avg=2.08\n",
            "[415 | 1024.74] loss=1.80 avg=2.08\n",
            "[416 | 1027.14] loss=2.02 avg=2.08\n",
            "[417 | 1029.53] loss=2.07 avg=2.08\n",
            "[418 | 1031.93] loss=2.07 avg=2.08\n",
            "[419 | 1034.33] loss=1.76 avg=2.08\n",
            "[420 | 1036.73] loss=2.15 avg=2.08\n",
            "[421 | 1039.13] loss=2.31 avg=2.08\n",
            "[422 | 1041.52] loss=2.11 avg=2.08\n",
            "[423 | 1043.92] loss=1.92 avg=2.08\n",
            "[424 | 1046.32] loss=2.25 avg=2.08\n",
            "[425 | 1048.71] loss=1.75 avg=2.08\n",
            "[426 | 1051.11] loss=2.06 avg=2.08\n",
            "[427 | 1053.49] loss=1.89 avg=2.08\n",
            "[428 | 1055.88] loss=1.74 avg=2.07\n",
            "[429 | 1058.27] loss=2.10 avg=2.07\n",
            "[430 | 1060.64] loss=2.53 avg=2.08\n",
            "[431 | 1063.03] loss=2.15 avg=2.08\n",
            "[432 | 1065.42] loss=2.00 avg=2.08\n",
            "[433 | 1067.80] loss=1.85 avg=2.07\n",
            "[434 | 1070.19] loss=2.23 avg=2.08\n",
            "[435 | 1072.57] loss=1.85 avg=2.07\n",
            "[436 | 1074.96] loss=1.92 avg=2.07\n",
            "[437 | 1077.35] loss=2.19 avg=2.07\n",
            "[438 | 1079.73] loss=1.89 avg=2.07\n",
            "[439 | 1082.12] loss=1.78 avg=2.07\n",
            "[440 | 1084.51] loss=2.17 avg=2.07\n",
            "[441 | 1086.89] loss=1.81 avg=2.07\n",
            "[442 | 1089.27] loss=1.77 avg=2.06\n",
            "[443 | 1091.65] loss=2.19 avg=2.07\n",
            "[444 | 1094.04] loss=2.00 avg=2.06\n",
            "[445 | 1096.42] loss=2.19 avg=2.07\n",
            "[446 | 1098.80] loss=1.96 avg=2.06\n",
            "[447 | 1101.18] loss=1.95 avg=2.06\n",
            "[448 | 1103.56] loss=1.92 avg=2.06\n",
            "[449 | 1105.95] loss=2.03 avg=2.06\n",
            "[450 | 1108.32] loss=2.08 avg=2.06\n",
            "[451 | 1110.70] loss=1.96 avg=2.06\n",
            "[452 | 1113.08] loss=1.84 avg=2.06\n",
            "[453 | 1115.47] loss=2.20 avg=2.06\n",
            "[454 | 1117.85] loss=2.18 avg=2.06\n",
            "[455 | 1120.22] loss=2.01 avg=2.06\n",
            "[456 | 1122.60] loss=2.28 avg=2.06\n",
            "[457 | 1124.97] loss=1.84 avg=2.06\n",
            "[458 | 1127.35] loss=2.01 avg=2.06\n",
            "[459 | 1129.73] loss=1.85 avg=2.06\n",
            "[460 | 1132.11] loss=1.93 avg=2.06\n",
            "[461 | 1134.48] loss=2.06 avg=2.06\n",
            "[462 | 1136.85] loss=2.09 avg=2.06\n",
            "[463 | 1139.24] loss=2.23 avg=2.06\n",
            "[464 | 1141.62] loss=1.85 avg=2.06\n",
            "[465 | 1144.00] loss=1.92 avg=2.06\n",
            "[466 | 1146.38] loss=2.14 avg=2.06\n",
            "[467 | 1148.77] loss=1.75 avg=2.05\n",
            "[468 | 1151.16] loss=1.94 avg=2.05\n",
            "[469 | 1153.54] loss=1.99 avg=2.05\n",
            "[470 | 1155.93] loss=1.91 avg=2.05\n",
            "[471 | 1158.32] loss=2.24 avg=2.05\n",
            "[472 | 1160.70] loss=1.92 avg=2.05\n",
            "[473 | 1163.09] loss=1.84 avg=2.05\n",
            "[474 | 1165.47] loss=2.00 avg=2.05\n",
            "[475 | 1167.85] loss=1.96 avg=2.05\n",
            "[476 | 1170.24] loss=1.85 avg=2.04\n",
            "[477 | 1172.62] loss=1.68 avg=2.04\n",
            "[478 | 1175.01] loss=1.87 avg=2.04\n",
            "[479 | 1177.40] loss=1.83 avg=2.04\n",
            "[480 | 1179.78] loss=1.88 avg=2.04\n",
            "[481 | 1182.18] loss=1.93 avg=2.03\n",
            "[482 | 1184.57] loss=2.32 avg=2.04\n",
            "[483 | 1186.96] loss=1.88 avg=2.04\n",
            "[484 | 1189.36] loss=1.65 avg=2.03\n",
            "[485 | 1191.76] loss=2.19 avg=2.03\n",
            "[486 | 1194.16] loss=1.72 avg=2.03\n",
            "[487 | 1196.54] loss=1.64 avg=2.03\n",
            "[488 | 1198.93] loss=2.24 avg=2.03\n",
            "[489 | 1201.32] loss=1.90 avg=2.03\n",
            "[490 | 1203.72] loss=1.97 avg=2.03\n",
            "[491 | 1206.11] loss=1.94 avg=2.03\n",
            "[492 | 1208.52] loss=1.86 avg=2.02\n",
            "[493 | 1210.92] loss=2.07 avg=2.03\n",
            "[494 | 1213.32] loss=2.18 avg=2.03\n",
            "[495 | 1215.71] loss=2.15 avg=2.03\n",
            "[496 | 1218.13] loss=2.11 avg=2.03\n",
            "[497 | 1220.54] loss=1.74 avg=2.03\n",
            "[498 | 1222.93] loss=1.77 avg=2.02\n",
            "[499 | 1225.33] loss=1.78 avg=2.02\n",
            "[500 | 1227.74] loss=1.76 avg=2.02\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "New in version 3.2.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timeout\n",
            "A subclass that can be initialized before or all of the loop’s\n",
            "interval, except when the time.interval and time.target\n",
            "items have been set.\n",
            "\n",
            "Deprecated since version 3.4.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Thread\n",
            "A subclass that can be initialized all the time and a single thread when\n",
            "the time.wait() object is invoked.\n",
            "Note that in order to have threads execute at the same time, one must\n",
            "immediately call the base class with a timeout of seconds, which\n",
            "can be delayed by calling run_thread().\n",
            "\n",
            "Deprecated since version 3.6.\n",
            "\n",
            "\n",
            "\n",
            "See also\n",
            "\n",
            "Thread for concurrent tasks (threaded.run_thread()) and\n",
            "interloop.time().\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timeout.wait()\n",
            "A subclass that raises an exception when the loop’s time.interval\n",
            "item has exceeded the time.sleep() time, set\n",
            "timeout, or any number of other timeouts at most once.  The\n",
            "time.sleep() timeouts occur as the result of a combination of\n",
            "the time.sleep() timeout and the event\n",
            "\n",
            "\n",
            "See also\n",
            "ThreadingError (as a time.sleep() exception)\n",
            "and Thread.sleepTimeoutError\n",
            "for more information on the internality of timedelta exceptions.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timeout.sleep(timeout)\n",
            "A subprocess that waits until timeout is reached while waiting\n",
            "for a timeout signal handler to be run. During the wait, this\n",
            "process raises an exception if it is caught (i.e., it is not\n",
            "able to signal or raise a timeout).  This raises\n",
            "Thread.sleep() for no particular reason.\n",
            "The wait() method allows passing an argument, such as an integer\n",
            "starting with -1 or 1, depending on the timeout parameter.  The\n",
            "argument must occur before the time.sleep() method.\n",
            "This is to ensure that the timeout isn’t interrupted, as long as\n",
            "it is an integer or non-integer which isn’t interrupted during a\n",
            "wait.\n",
            "\n",
            "Warning\n",
            "If the timeout argument is not supplied to the __future__ module of\n",
            "__import__(), a timeout can’t be specified in the\n",
            "__future__() attribute of Timeout’s __future__()\n",
            "method.  If timeout is supplied to a class which\n",
            "is not part of the class hierarchy, the timeout attribute will be passed\n",
            "as None.\n",
            "\n",
            "Changed in version 3.7: Accepts a non-zero timeout parameter.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timer\n",
            "A subclass that can be initialized at any time and run concurrently when\n",
            "the time.stop() method is called. It can, for example, be called from\n",
            "a threaded instance and then called from a timer.\n",
            "\n",
            "Deprecated since version 3.6 and will be removed in version 4.0.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timer(type=None)\n",
            "A subclass of timer which can be initialized at any time and run concurrently when\n",
            "the timer object is called from a threaded\n",
            "instance that has a timer object of the same\n",
            "type.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timer(type=None, time=None, start_thread=None, stop_thread=None, start_thread_start=None)\n",
            "A subclass that can be initialized at any time and run concurrently when\n",
            "the timer object is called from a threaded\n",
            "instance that has a timer object of the same type.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.Timer(type=None, time=None, start_thread=None, stop_thread=None, start_thread_stop=None, stop_thread_stop=None)\n",
            "A subclass which can be initialized at any time and run concurrently when\n",
            "the timer object is called from a threaded\n",
            "instance that has a timer object of the same type. See\n",
            "classmethod for a more detailed description.\n",
            "The start_thread/stop_thread arguments describe the start of scheduled\n",
            "timeouts.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class threading.TimeInSeconds(seconds=None, start_time=None, stop_time=None, time_in_sec=None, stop_time_from_start=None)\n",
            "A subclass that can be initialized at any time and run concurrently when\n",
            "the time.introt()) and clock() methods are\n",
            "called.\n",
            "The time_in_sec and stop_time_from_start arguments\n",
            "describe where clock() should stop at when time.is_now()\n",
            "is called and optionally start_time. When timedelta(seconds, start_time=start_time) is not\n",
            "available, a timeout will be provided for the timedelta(seconds, stop_time=stop_time)\n",
            "resulting from start_time\n",
            "\n",
            "[501 | 1240.88] loss=2.09 avg=2.02\n",
            "[502 | 1243.29] loss=2.18 avg=2.02\n",
            "[503 | 1245.69] loss=1.73 avg=2.02\n",
            "[504 | 1248.08] loss=1.89 avg=2.02\n",
            "[505 | 1250.47] loss=2.16 avg=2.02\n",
            "[506 | 1252.86] loss=1.87 avg=2.02\n",
            "[507 | 1255.25] loss=2.03 avg=2.02\n",
            "[508 | 1257.64] loss=2.07 avg=2.02\n",
            "[509 | 1260.02] loss=1.90 avg=2.02\n",
            "[510 | 1262.41] loss=1.84 avg=2.01\n",
            "[511 | 1264.79] loss=2.05 avg=2.01\n",
            "[512 | 1267.18] loss=2.01 avg=2.01\n",
            "[513 | 1269.55] loss=2.15 avg=2.02\n",
            "[514 | 1271.94] loss=2.08 avg=2.02\n",
            "[515 | 1274.33] loss=2.02 avg=2.02\n",
            "[516 | 1276.70] loss=2.08 avg=2.02\n",
            "[517 | 1279.08] loss=1.97 avg=2.02\n",
            "[518 | 1281.45] loss=1.80 avg=2.01\n",
            "[519 | 1283.83] loss=1.80 avg=2.01\n",
            "[520 | 1286.21] loss=2.04 avg=2.01\n",
            "[521 | 1288.60] loss=1.80 avg=2.01\n",
            "[522 | 1290.96] loss=2.10 avg=2.01\n",
            "[523 | 1293.34] loss=2.05 avg=2.01\n",
            "[524 | 1295.71] loss=1.65 avg=2.01\n",
            "[525 | 1298.10] loss=1.63 avg=2.00\n",
            "[526 | 1300.48] loss=2.05 avg=2.00\n",
            "[527 | 1302.86] loss=2.22 avg=2.01\n",
            "[528 | 1305.24] loss=1.80 avg=2.00\n",
            "[529 | 1307.62] loss=2.06 avg=2.01\n",
            "[530 | 1309.99] loss=2.14 avg=2.01\n",
            "[531 | 1312.38] loss=2.15 avg=2.01\n",
            "[532 | 1314.77] loss=1.77 avg=2.01\n",
            "[533 | 1317.16] loss=1.96 avg=2.01\n",
            "[534 | 1319.54] loss=2.30 avg=2.01\n",
            "[535 | 1321.93] loss=2.10 avg=2.01\n",
            "[536 | 1324.33] loss=1.64 avg=2.01\n",
            "[537 | 1326.69] loss=2.23 avg=2.01\n",
            "[538 | 1329.07] loss=1.98 avg=2.01\n",
            "[539 | 1331.44] loss=1.86 avg=2.01\n",
            "[540 | 1333.82] loss=1.94 avg=2.01\n",
            "[541 | 1336.20] loss=1.89 avg=2.00\n",
            "[542 | 1338.58] loss=1.90 avg=2.00\n",
            "[543 | 1340.96] loss=1.87 avg=2.00\n",
            "[544 | 1343.33] loss=1.62 avg=2.00\n",
            "[545 | 1345.72] loss=1.80 avg=2.00\n",
            "[546 | 1348.10] loss=2.02 avg=2.00\n",
            "[547 | 1350.48] loss=2.10 avg=2.00\n",
            "[548 | 1352.85] loss=1.73 avg=1.99\n",
            "[549 | 1355.24] loss=2.18 avg=2.00\n",
            "[550 | 1357.63] loss=1.71 avg=1.99\n",
            "[551 | 1360.01] loss=2.05 avg=1.99\n",
            "[552 | 1362.40] loss=1.59 avg=1.99\n",
            "[553 | 1364.79] loss=2.02 avg=1.99\n",
            "[554 | 1367.17] loss=2.00 avg=1.99\n",
            "[555 | 1369.56] loss=2.29 avg=1.99\n",
            "[556 | 1371.95] loss=2.19 avg=2.00\n",
            "[557 | 1374.33] loss=1.67 avg=1.99\n",
            "[558 | 1376.72] loss=1.90 avg=1.99\n",
            "[559 | 1379.10] loss=2.08 avg=1.99\n",
            "[560 | 1381.48] loss=1.82 avg=1.99\n",
            "[561 | 1383.87] loss=1.83 avg=1.99\n",
            "[562 | 1386.25] loss=1.87 avg=1.99\n",
            "[563 | 1388.64] loss=1.90 avg=1.99\n",
            "[564 | 1391.02] loss=2.06 avg=1.99\n",
            "[565 | 1393.41] loss=1.88 avg=1.99\n",
            "[566 | 1395.79] loss=2.15 avg=1.99\n",
            "[567 | 1398.18] loss=2.37 avg=1.99\n",
            "[568 | 1400.57] loss=1.88 avg=1.99\n",
            "[569 | 1402.94] loss=1.52 avg=1.99\n",
            "[570 | 1405.32] loss=1.83 avg=1.98\n",
            "[571 | 1407.71] loss=1.73 avg=1.98\n",
            "[572 | 1410.09] loss=2.09 avg=1.98\n",
            "[573 | 1412.48] loss=2.32 avg=1.99\n",
            "[574 | 1414.87] loss=2.10 avg=1.99\n",
            "[575 | 1417.25] loss=2.06 avg=1.99\n",
            "[576 | 1419.63] loss=1.64 avg=1.98\n",
            "[577 | 1422.02] loss=2.07 avg=1.99\n",
            "[578 | 1424.40] loss=1.67 avg=1.98\n",
            "[579 | 1426.80] loss=1.98 avg=1.98\n",
            "[580 | 1429.18] loss=2.00 avg=1.98\n",
            "[581 | 1431.57] loss=2.18 avg=1.98\n",
            "[582 | 1433.95] loss=2.11 avg=1.99\n",
            "[583 | 1436.33] loss=1.58 avg=1.98\n",
            "[584 | 1438.72] loss=1.82 avg=1.98\n",
            "[585 | 1441.09] loss=1.87 avg=1.98\n",
            "[586 | 1443.48] loss=2.12 avg=1.98\n",
            "[587 | 1445.87] loss=1.72 avg=1.98\n",
            "[588 | 1448.26] loss=1.68 avg=1.97\n",
            "[589 | 1450.65] loss=1.83 avg=1.97\n",
            "[590 | 1453.03] loss=1.99 avg=1.97\n",
            "[591 | 1455.41] loss=1.99 avg=1.97\n",
            "[592 | 1457.80] loss=1.99 avg=1.97\n",
            "[593 | 1460.18] loss=1.73 avg=1.97\n",
            "[594 | 1462.57] loss=1.86 avg=1.97\n",
            "[595 | 1464.95] loss=1.88 avg=1.97\n",
            "[596 | 1467.34] loss=1.59 avg=1.97\n",
            "[597 | 1469.72] loss=1.74 avg=1.96\n",
            "[598 | 1472.09] loss=1.95 avg=1.96\n",
            "[599 | 1474.47] loss=2.24 avg=1.97\n",
            "[600 | 1476.86] loss=1.87 avg=1.97\n",
            "======== SAMPLE 1 ========\n",
            " supported the\n",
            "command.  The result will always be True if you passed\n",
            "a command argument.\n",
            "\n",
            "\n",
            "\n",
            "command.aisok()\n",
            "Return True if the argument was actually defined by the\n",
            "command; otherwise return False.  If true, the\n",
            "command is defined with no arguments, and therefore there\n",
            "is no need to define a new command with arguments.\n",
            "This function is implemented by the standard library’s\n",
            "command family interface.  This is the recommended form of\n",
            "command with the same name.\n",
            "\n",
            "Changed in version 3.8: The aisok() argument has a meaning when it is a\n",
            "symbol of the option flag in the standard Python parser.  This function\n",
            "defines an argument for every invocation thereof.  Also, “the value of the a**3”\n",
            "argument to a built-in set of built-in options can be used to\n",
            "provide an alternate meaning for a built-in argument that can be used\n",
            "by setdefault() and setfrozen().\n",
            "\n",
            "Changed in version 3.7: A built-in option argument can be used instead of an alternative\n",
            "(as if the argument were an option).  There is no alternative to\n",
            "any of the options defined in the standard Python parser, and only\n",
            "the default is the default one.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "command.ascmdline(cmd, opt_, opt_flags)\n",
            "Compute a command-line option-line option-line option by executing\n",
            "command.  cmd can be any of the options defined in the standard Python\n",
            "interpreter.  opt_flags are the required flags for the option to\n",
            "be executed. Example of an opt_flag field:\n",
            "\n",
            "\n",
            "command.ascmdline(cmd, opt_flags, opt_cmd)\n",
            "Convert cmd to a command-line option-line option by executing the\n",
            "option.  This option-line option cannot appear\n",
            "within the option definition.\n",
            "An optional opt_flags field is defined when opt_flags is not\n",
            "empty.  This causes the command to be executed by default, but this can\n",
            "easily be set with the command keyword.  For example, opt_cmd is\n",
            "supplied when the option option is executed:\n",
            "\n",
            "\n",
            "<command>                                                       *\n",
            "                                                     --\n",
            "                                                            --\n",
            "                                                    *\n",
            "                                               --\n",
            "\n",
            "\n",
            "The arguments passed to the command will be returned to the\n",
            "command if desired.  The default interpretation of these\n",
            "arguments is that the default values will be passed to the new\n",
            "option when the default is defined.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                  \n",
            "                           \n",
            "                                 *\n",
            "                                        --\n",
            "                                                           --\n",
            "                                                   --\n",
            "  \n",
            "\n",
            "[601 | 1490.00] loss=2.01 avg=1.97\n",
            "[602 | 1492.40] loss=1.91 avg=1.97\n",
            "[603 | 1494.78] loss=1.90 avg=1.96\n",
            "[604 | 1497.17] loss=2.07 avg=1.97\n",
            "[605 | 1499.55] loss=1.79 avg=1.96\n",
            "[606 | 1501.95] loss=1.89 avg=1.96\n",
            "[607 | 1504.33] loss=1.95 avg=1.96\n",
            "[608 | 1506.72] loss=1.74 avg=1.96\n",
            "[609 | 1509.11] loss=1.99 avg=1.96\n",
            "[610 | 1511.51] loss=1.98 avg=1.96\n",
            "[611 | 1513.89] loss=2.26 avg=1.96\n",
            "[612 | 1516.28] loss=2.08 avg=1.97\n",
            "[613 | 1518.66] loss=1.90 avg=1.96\n",
            "[614 | 1521.04] loss=1.69 avg=1.96\n",
            "[615 | 1523.43] loss=1.83 avg=1.96\n",
            "[616 | 1525.81] loss=2.25 avg=1.96\n",
            "[617 | 1528.20] loss=1.73 avg=1.96\n",
            "[618 | 1530.59] loss=1.71 avg=1.96\n",
            "[619 | 1532.97] loss=2.12 avg=1.96\n",
            "[620 | 1535.36] loss=1.81 avg=1.96\n",
            "[621 | 1537.74] loss=1.55 avg=1.95\n",
            "[622 | 1540.12] loss=1.81 avg=1.95\n",
            "[623 | 1542.51] loss=2.09 avg=1.95\n",
            "[624 | 1544.90] loss=1.58 avg=1.95\n",
            "[625 | 1547.28] loss=1.92 avg=1.95\n",
            "[626 | 1549.66] loss=2.03 avg=1.95\n",
            "[627 | 1552.05] loss=1.76 avg=1.95\n",
            "[628 | 1554.43] loss=1.78 avg=1.95\n",
            "[629 | 1556.81] loss=1.92 avg=1.95\n",
            "[630 | 1559.20] loss=2.06 avg=1.95\n",
            "[631 | 1561.59] loss=1.86 avg=1.95\n",
            "[632 | 1563.96] loss=1.88 avg=1.95\n",
            "[633 | 1566.35] loss=2.08 avg=1.95\n",
            "[634 | 1568.74] loss=1.88 avg=1.95\n",
            "[635 | 1571.12] loss=1.90 avg=1.95\n",
            "[636 | 1573.51] loss=2.24 avg=1.95\n",
            "[637 | 1575.91] loss=1.68 avg=1.95\n",
            "[638 | 1578.29] loss=1.76 avg=1.95\n",
            "[639 | 1580.68] loss=1.60 avg=1.94\n",
            "[640 | 1583.07] loss=1.77 avg=1.94\n",
            "[641 | 1585.46] loss=1.81 avg=1.94\n",
            "[642 | 1587.85] loss=1.91 avg=1.94\n",
            "[643 | 1590.24] loss=2.04 avg=1.94\n",
            "[644 | 1592.62] loss=2.00 avg=1.94\n",
            "[645 | 1595.01] loss=1.88 avg=1.94\n",
            "[646 | 1597.40] loss=1.77 avg=1.94\n",
            "[647 | 1599.79] loss=2.11 avg=1.94\n",
            "[648 | 1602.17] loss=2.05 avg=1.94\n",
            "[649 | 1604.56] loss=1.89 avg=1.94\n",
            "[650 | 1606.95] loss=1.94 avg=1.94\n",
            "[651 | 1609.33] loss=1.59 avg=1.94\n",
            "[652 | 1611.72] loss=1.88 avg=1.94\n",
            "[653 | 1614.11] loss=1.68 avg=1.93\n",
            "[654 | 1616.50] loss=1.71 avg=1.93\n",
            "[655 | 1618.89] loss=1.99 avg=1.93\n",
            "[656 | 1621.27] loss=1.83 avg=1.93\n",
            "[657 | 1623.66] loss=2.07 avg=1.93\n",
            "[658 | 1626.04] loss=1.91 avg=1.93\n",
            "[659 | 1628.41] loss=1.90 avg=1.93\n",
            "[660 | 1630.79] loss=1.98 avg=1.93\n",
            "[661 | 1633.19] loss=1.63 avg=1.93\n",
            "[662 | 1635.56] loss=1.71 avg=1.93\n",
            "[663 | 1637.95] loss=1.96 avg=1.93\n",
            "[664 | 1640.34] loss=2.11 avg=1.93\n",
            "[665 | 1642.73] loss=1.74 avg=1.93\n",
            "[666 | 1645.13] loss=1.94 avg=1.93\n",
            "[667 | 1647.52] loss=2.02 avg=1.93\n",
            "[668 | 1649.91] loss=1.65 avg=1.93\n",
            "[669 | 1652.30] loss=1.77 avg=1.92\n",
            "[670 | 1654.68] loss=1.80 avg=1.92\n",
            "[671 | 1657.08] loss=1.57 avg=1.92\n",
            "[672 | 1659.46] loss=2.15 avg=1.92\n",
            "[673 | 1661.85] loss=1.94 avg=1.92\n",
            "[674 | 1664.24] loss=2.52 avg=1.93\n",
            "[675 | 1666.62] loss=1.71 avg=1.93\n",
            "[676 | 1669.00] loss=1.87 avg=1.93\n",
            "[677 | 1671.39] loss=2.07 avg=1.93\n",
            "[678 | 1673.77] loss=1.86 avg=1.93\n",
            "[679 | 1676.15] loss=1.84 avg=1.92\n",
            "[680 | 1678.54] loss=2.02 avg=1.93\n",
            "[681 | 1680.92] loss=1.64 avg=1.92\n",
            "[682 | 1683.32] loss=1.89 avg=1.92\n",
            "[683 | 1685.71] loss=2.10 avg=1.92\n",
            "[684 | 1688.10] loss=2.19 avg=1.93\n",
            "[685 | 1690.48] loss=1.64 avg=1.92\n",
            "[686 | 1692.87] loss=2.30 avg=1.93\n",
            "[687 | 1695.27] loss=1.85 avg=1.93\n",
            "[688 | 1697.66] loss=1.67 avg=1.92\n",
            "[689 | 1700.05] loss=1.57 avg=1.92\n",
            "[690 | 1702.44] loss=1.78 avg=1.92\n",
            "[691 | 1704.83] loss=1.97 avg=1.92\n",
            "[692 | 1707.22] loss=1.67 avg=1.92\n",
            "[693 | 1709.60] loss=1.75 avg=1.92\n",
            "[694 | 1711.99] loss=1.84 avg=1.92\n",
            "[695 | 1714.38] loss=1.77 avg=1.91\n",
            "[696 | 1716.76] loss=1.70 avg=1.91\n",
            "[697 | 1719.15] loss=1.86 avg=1.91\n",
            "[698 | 1721.54] loss=1.90 avg=1.91\n",
            "[699 | 1723.93] loss=1.72 avg=1.91\n",
            "[700 | 1726.31] loss=1.58 avg=1.91\n",
            "======== SAMPLE 1 ========\n",
            " IP\n",
            ">>> f = b\"*\"\n",
            ">>> f.name\n",
            "\"\"\n",
            ">>> f.name\n",
            "\"\"\n",
            ">>> f.value\n",
            "9\n",
            ">>> f.size\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ">>>>>> class Point(b'a')\n",
            "...      x = b\"**\"\n",
            ">>> print(x)                   \n",
            "Point()\n",
            "x = b\"%7d\" % 10\n",
            ">>> x.name\n",
            "\"Raymond Wilson\"\n",
            ">>>                   [Point().__name__]\n",
            "Point()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ">>> # of arguments:\n",
            "    x, y = b\"**\"\n",
            ">>> z = Point()                         # compute x's value\n",
            ">>> x, y = b\"**\"\n",
            ">>> z += x\n",
            ">>> z < 3                     # compute y's value\n",
            "Point()\n",
            "x = b\"%3d\" % z\n",
            "\n",
            ">>> x = b\"%1d\" % (x / 3)\n",
            "\n",
            ">>> d = Point()                       # compute d's value\n",
            ">>> d < 3  # compute d's size\n",
            "Bool(d)\n",
            "Point()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2.compile(size, d)\n",
            "Computation function\n",
            "\n",
            "Note\n",
            "In bzip2, the size parameter is the absolute value, not an\n",
            "integer.  The d parameter is an integer that must be\n",
            "concatenated onto a dictionary to create a new\n",
            "dictionary attribute.  Note that this only applies to\n",
            "long and/or fixed length arrays, not any other data types that have\n",
            "long names.  Note that this does not apply when you create an iterable\n",
            "that doesn’t fit into a new sequence.  For example,\n",
            "if you have a fixed length list, then d must be equal to the length of\n",
            "the list object.\n",
            "The size parameter determines whether d is an integer\n",
            "or not.  The size of the dictionary is not an attribute, it\n",
            "is a dictionary created by compile() and passed to\n",
            "bzip2.\n",
            "\n",
            "Note\n",
            "In bzip2, the size parameter is the absolute value, not an\n",
            "integer.  The d parameter is an integer that must be\n",
            "concatenated onto a dictionary to create a new\n",
            "dictionary attribute.  For example,\n",
            "if you have a fixed length list, then d must be equal to the length of\n",
            "the list object.\n",
            "The size parameter determines whether d is an integer\n",
            "or not.\n",
            "\n",
            "New in version 3.7.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2.compress(filename, maxsize=-1, dst=-1)\n",
            "Compress the filename and return the resulting\n",
            "dictionary data.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ">>> from random import randrange, compress\n",
            ">>> d = randrange(0, 4096).sample(10)  #sample the list on top\n",
            ">>> for i in range(10):\n",
            "...    d.append(i)\n",
            ">>> d    = '\\030000'  # sample the list on top\n",
            ">>> d    = '.\\t'\n",
            ">>> d                # sample the list on top of an array\n",
            ">>> d    = '.\\t'\n",
            ">>> d                # sample the list of lists on top of a list\n",
            ">>> d    = '\\t'\n",
            ">>> d                     # print some output\n",
            ">>> d    = '.\\t'\n",
            ">>> d                   # print some output\n",
            ">>> d   = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> d     = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> d    = '.\\t'\n",
            ">>> b\"\\t\"\n",
            "b\"\\t\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Note\n",
            "In bzip2, the size parameter is the absolute value, not an\n",
            "integer.  The size of the dictionary is not an attribute, it is\n",
            "a dictionary created by compile() and passed to\n",
            "\n",
            "[701 | 1739.50] loss=1.72 avg=1.90\n",
            "[702 | 1741.91] loss=1.84 avg=1.90\n",
            "[703 | 1744.30] loss=1.86 avg=1.90\n",
            "[704 | 1746.68] loss=1.83 avg=1.90\n",
            "[705 | 1749.08] loss=2.03 avg=1.90\n",
            "[706 | 1751.48] loss=2.12 avg=1.91\n",
            "[707 | 1753.86] loss=2.05 avg=1.91\n",
            "[708 | 1756.25] loss=1.67 avg=1.90\n",
            "[709 | 1758.65] loss=2.03 avg=1.91\n",
            "[710 | 1761.03] loss=1.61 avg=1.90\n",
            "[711 | 1763.42] loss=1.95 avg=1.90\n",
            "[712 | 1765.81] loss=1.73 avg=1.90\n",
            "[713 | 1768.21] loss=2.14 avg=1.90\n",
            "[714 | 1770.60] loss=1.99 avg=1.90\n",
            "[715 | 1772.99] loss=2.11 avg=1.91\n",
            "[716 | 1775.37] loss=1.56 avg=1.90\n",
            "[717 | 1777.76] loss=2.06 avg=1.90\n",
            "[718 | 1780.16] loss=1.73 avg=1.90\n",
            "[719 | 1782.55] loss=1.61 avg=1.90\n",
            "[720 | 1784.94] loss=2.06 avg=1.90\n",
            "[721 | 1787.34] loss=2.25 avg=1.91\n",
            "[722 | 1789.73] loss=1.87 avg=1.90\n",
            "[723 | 1792.11] loss=1.80 avg=1.90\n",
            "[724 | 1794.50] loss=1.60 avg=1.90\n",
            "[725 | 1796.88] loss=1.87 avg=1.90\n",
            "[726 | 1799.27] loss=1.67 avg=1.90\n",
            "[727 | 1801.66] loss=1.79 avg=1.90\n",
            "[728 | 1804.04] loss=1.95 avg=1.90\n",
            "[729 | 1806.43] loss=2.04 avg=1.90\n",
            "[730 | 1808.81] loss=1.65 avg=1.90\n",
            "[731 | 1811.20] loss=1.83 avg=1.90\n",
            "[732 | 1813.58] loss=2.00 avg=1.90\n",
            "[733 | 1815.97] loss=2.21 avg=1.90\n",
            "[734 | 1818.35] loss=1.75 avg=1.90\n",
            "[735 | 1820.75] loss=1.76 avg=1.90\n",
            "[736 | 1823.13] loss=1.87 avg=1.90\n",
            "[737 | 1825.53] loss=1.77 avg=1.90\n",
            "[738 | 1827.92] loss=1.83 avg=1.89\n",
            "[739 | 1830.30] loss=2.02 avg=1.90\n",
            "[740 | 1832.69] loss=1.99 avg=1.90\n",
            "[741 | 1835.07] loss=2.12 avg=1.90\n",
            "[742 | 1837.46] loss=1.68 avg=1.90\n",
            "[743 | 1839.86] loss=1.65 avg=1.89\n",
            "[744 | 1842.25] loss=1.92 avg=1.90\n",
            "[745 | 1844.63] loss=2.17 avg=1.90\n",
            "[746 | 1847.02] loss=1.88 avg=1.90\n",
            "[747 | 1849.40] loss=1.62 avg=1.89\n",
            "[748 | 1851.79] loss=1.71 avg=1.89\n",
            "[749 | 1854.18] loss=1.77 avg=1.89\n",
            "[750 | 1856.58] loss=1.80 avg=1.89\n",
            "[751 | 1858.97] loss=1.94 avg=1.89\n",
            "[752 | 1861.35] loss=1.89 avg=1.89\n",
            "[753 | 1863.74] loss=1.73 avg=1.89\n",
            "[754 | 1866.14] loss=1.69 avg=1.89\n",
            "[755 | 1868.53] loss=1.85 avg=1.89\n",
            "[756 | 1870.92] loss=1.95 avg=1.89\n",
            "[757 | 1873.30] loss=1.93 avg=1.89\n",
            "[758 | 1875.69] loss=1.77 avg=1.89\n",
            "[759 | 1878.08] loss=1.88 avg=1.89\n",
            "[760 | 1880.47] loss=1.87 avg=1.89\n",
            "[761 | 1882.86] loss=1.53 avg=1.88\n",
            "[762 | 1885.25] loss=1.81 avg=1.88\n",
            "[763 | 1887.63] loss=1.72 avg=1.88\n",
            "[764 | 1890.00] loss=1.90 avg=1.88\n",
            "[765 | 1892.39] loss=1.94 avg=1.88\n",
            "[766 | 1894.78] loss=1.62 avg=1.88\n",
            "[767 | 1897.18] loss=1.91 avg=1.88\n",
            "[768 | 1899.55] loss=1.88 avg=1.88\n",
            "[769 | 1901.94] loss=1.62 avg=1.88\n",
            "[770 | 1904.33] loss=1.68 avg=1.87\n",
            "[771 | 1906.71] loss=1.76 avg=1.87\n",
            "[772 | 1909.10] loss=1.55 avg=1.87\n",
            "[773 | 1911.48] loss=1.83 avg=1.87\n",
            "[774 | 1913.88] loss=1.75 avg=1.87\n",
            "[775 | 1916.26] loss=2.05 avg=1.87\n",
            "[776 | 1918.64] loss=1.72 avg=1.87\n",
            "[777 | 1921.03] loss=1.66 avg=1.87\n",
            "[778 | 1923.41] loss=1.81 avg=1.87\n",
            "[779 | 1925.80] loss=1.86 avg=1.87\n",
            "[780 | 1928.18] loss=1.67 avg=1.86\n",
            "[781 | 1930.57] loss=1.93 avg=1.87\n",
            "[782 | 1932.95] loss=1.94 avg=1.87\n",
            "[783 | 1935.33] loss=1.88 avg=1.87\n",
            "[784 | 1937.72] loss=1.81 avg=1.87\n",
            "[785 | 1940.11] loss=1.99 avg=1.87\n",
            "[786 | 1942.51] loss=1.62 avg=1.86\n",
            "[787 | 1944.89] loss=1.95 avg=1.87\n",
            "[788 | 1947.26] loss=1.94 avg=1.87\n",
            "[789 | 1949.64] loss=1.65 avg=1.86\n",
            "[790 | 1952.03] loss=1.78 avg=1.86\n",
            "[791 | 1954.40] loss=1.83 avg=1.86\n",
            "[792 | 1956.79] loss=1.71 avg=1.86\n",
            "[793 | 1959.17] loss=1.79 avg=1.86\n",
            "[794 | 1961.56] loss=2.22 avg=1.86\n",
            "[795 | 1963.95] loss=1.47 avg=1.86\n",
            "[796 | 1966.35] loss=1.81 avg=1.86\n",
            "[797 | 1968.74] loss=2.00 avg=1.86\n",
            "[798 | 1971.13] loss=1.80 avg=1.86\n",
            "[799 | 1973.51] loss=1.72 avg=1.86\n",
            "[800 | 1975.90] loss=1.53 avg=1.86\n",
            "======== SAMPLE 1 ========\n",
            " no\n",
            "more.  You only get one exception: The first is a reference to the\n",
            "previous thread.\n",
            "The third exception is still documented below.  However, it is\n",
            "still called a thread-related exception.\n",
            "The Python interpreter uses the built-in threading module (as defined in The\n",
            "standard library) to serialize the data to a file socket, and it does this by\n",
            "using the socket module. Thus all exceptions are reported in file handles\n",
            "and namespaces, rather than being contained in a file.\n",
            "\n",
            "\n",
            "See also\n",
            "\n",
            "Module threadingAn extension that provides support for threading.\n",
            "Examples of threading objects for example.\n",
            "The file handles in .pth files are:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CObject\n",
            "Object\n",
            "\n",
            "object.pfd\n",
            "\n",
            "object.lstp\n",
            "\n",
            "object.socketsize\n",
            "\n",
            "object.size\n",
            "\n",
            "object.popen\n",
            "\n",
            "object.fd\n",
            "\n",
            "object.pcharset\n",
            "String\n",
            "\n",
            "object.errors\n",
            "\n",
            "object.errorlevel\n",
            "The set of errors defined below.  This is\n",
            "default to one.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CObject & Objects\n",
            "The object.pfd\n",
            "Class\n",
            "BaseClass\n",
            "\n",
            "object.pcharset\n",
            "String\n",
            "\n",
            "object.errors\n",
            "Exception\n",
            "\n",
            "object.errorlevel\n",
            "The set of errors defined below.  This is\n",
            "default to one.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Class CObject\n",
            "object.Pcharset\n",
            "Value representing the value of the pcharset variable.\n",
            "\n",
            "The objects listed above all have the same type.\n",
            "\n",
            "\n",
            "Class methods\n",
            "These methods are all C methods that accept one argument. There is no\n",
            "standard way to get an equivalent type in your code; instead, it is the simplest\n",
            "form of assignment:\n",
            "\n",
            "\n",
            "class C(object):\n",
            "    type = C(\"Python\", object.pcharset)\n",
            "    ...\n",
            "    # prints                   ...\n",
            "    return new C(Type(\"class\", type.C_STRING))\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    PyArg_ParseTuple(\"--foo\", \"foo\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    PyArg_ParseTuple(\"--fancy\", \"for your own fancy pyc\");\n",
            "    return PyArg_ParseTuple(\"--main.py\", &\"foo.py\");\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Here is a typical Python program:\n",
            "\n",
            "\n",
            "#include <ctypes.h>\n",
            "#define PY_MAJOR\n",
            "#include <string.h>\n",
            "#include <cfile.h>\n",
            "#include <sys/types.h>\n",
            "#include <ctypes.h>\n",
            "#include \"type.h\"\n",
            "#include <string.h>\n",
            "#include <struct.h>\n",
            "#include \"type.h\"\n",
            "#include <file.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <int.h>\n",
            "#include <ctypes.h>\n",
            "#include <type.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <string.h>\n",
            "#include <class.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <type.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <class.h>\n",
            "#include <list.h>\n",
            "#include <string.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#define PY_MAJOR\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <ctypes.h>\n",
            "#include <int.h>\n",
            "#include <string.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <string.h>\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <string.h>\n",
            "#include <string.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "#include <ctypes.h>\n",
            "int Py_Bytes_FromString(PyObject *o)\n",
            "return-0;\n",
            "\n",
            "\n",
            "#else:\n",
            "\n",
            "#define PY_MAJOR\n",
            "#include <ctypes.h>\n",
            "#include <string.h>\n",
            "#include <\n",
            "\n",
            "[801 | 1989.09] loss=1.96 avg=1.86\n",
            "[802 | 1991.49] loss=1.83 avg=1.86\n",
            "[803 | 1993.88] loss=1.75 avg=1.86\n",
            "[804 | 1996.27] loss=1.72 avg=1.85\n",
            "[805 | 1998.66] loss=1.80 avg=1.85\n",
            "[806 | 2001.04] loss=1.77 avg=1.85\n",
            "[807 | 2003.43] loss=1.89 avg=1.85\n",
            "[808 | 2005.82] loss=1.83 avg=1.85\n",
            "[809 | 2008.21] loss=1.83 avg=1.85\n",
            "[810 | 2010.60] loss=1.77 avg=1.85\n",
            "[811 | 2012.98] loss=1.68 avg=1.85\n",
            "[812 | 2015.37] loss=1.73 avg=1.85\n",
            "[813 | 2017.76] loss=1.93 avg=1.85\n",
            "[814 | 2020.15] loss=1.95 avg=1.85\n",
            "[815 | 2022.54] loss=1.79 avg=1.85\n",
            "[816 | 2024.92] loss=1.72 avg=1.85\n",
            "[817 | 2027.31] loss=1.93 avg=1.85\n",
            "[818 | 2029.70] loss=1.80 avg=1.85\n",
            "[819 | 2032.08] loss=1.58 avg=1.85\n",
            "[820 | 2034.47] loss=1.86 avg=1.85\n",
            "[821 | 2036.86] loss=1.84 avg=1.85\n",
            "[822 | 2039.25] loss=1.38 avg=1.84\n",
            "[823 | 2041.63] loss=2.00 avg=1.84\n",
            "[824 | 2044.01] loss=1.46 avg=1.84\n",
            "[825 | 2046.40] loss=1.84 avg=1.84\n",
            "[826 | 2048.79] loss=1.90 avg=1.84\n",
            "[827 | 2051.17] loss=1.87 avg=1.84\n",
            "[828 | 2053.55] loss=1.73 avg=1.84\n",
            "[829 | 2055.94] loss=2.09 avg=1.84\n",
            "[830 | 2058.34] loss=1.92 avg=1.84\n",
            "[831 | 2060.72] loss=1.67 avg=1.84\n",
            "[832 | 2063.11] loss=1.70 avg=1.84\n",
            "[833 | 2065.49] loss=1.77 avg=1.84\n",
            "[834 | 2067.87] loss=1.75 avg=1.84\n",
            "[835 | 2070.25] loss=1.95 avg=1.84\n",
            "[836 | 2072.63] loss=1.63 avg=1.84\n",
            "[837 | 2075.02] loss=1.84 avg=1.84\n",
            "[838 | 2077.41] loss=1.88 avg=1.84\n",
            "[839 | 2079.80] loss=2.19 avg=1.84\n",
            "[840 | 2082.19] loss=1.54 avg=1.84\n",
            "[841 | 2084.58] loss=1.72 avg=1.84\n",
            "[842 | 2086.96] loss=1.80 avg=1.84\n",
            "[843 | 2089.35] loss=1.91 avg=1.84\n",
            "[844 | 2091.73] loss=1.91 avg=1.84\n",
            "[845 | 2094.12] loss=1.65 avg=1.84\n",
            "[846 | 2096.50] loss=1.92 avg=1.84\n",
            "[847 | 2098.89] loss=1.76 avg=1.84\n",
            "[848 | 2101.29] loss=1.95 avg=1.84\n",
            "[849 | 2103.67] loss=1.72 avg=1.84\n",
            "[850 | 2106.06] loss=1.63 avg=1.83\n",
            "[851 | 2108.44] loss=1.72 avg=1.83\n",
            "[852 | 2110.83] loss=1.64 avg=1.83\n",
            "[853 | 2113.21] loss=1.70 avg=1.83\n",
            "[854 | 2115.59] loss=1.99 avg=1.83\n",
            "[855 | 2117.97] loss=1.90 avg=1.83\n",
            "[856 | 2120.36] loss=1.89 avg=1.83\n",
            "[857 | 2122.75] loss=1.82 avg=1.83\n",
            "[858 | 2125.13] loss=1.78 avg=1.83\n",
            "[859 | 2127.51] loss=1.79 avg=1.83\n",
            "[860 | 2129.90] loss=2.16 avg=1.83\n",
            "[861 | 2132.28] loss=1.64 avg=1.83\n",
            "[862 | 2134.66] loss=1.63 avg=1.83\n",
            "[863 | 2137.05] loss=1.87 avg=1.83\n",
            "[864 | 2139.42] loss=2.13 avg=1.83\n",
            "[865 | 2141.80] loss=1.92 avg=1.83\n",
            "[866 | 2144.18] loss=1.52 avg=1.83\n",
            "[867 | 2146.57] loss=1.57 avg=1.83\n",
            "[868 | 2148.94] loss=1.84 avg=1.83\n",
            "[869 | 2151.31] loss=1.88 avg=1.83\n",
            "[870 | 2153.70] loss=1.74 avg=1.83\n",
            "[871 | 2156.07] loss=1.90 avg=1.83\n",
            "[872 | 2158.46] loss=2.04 avg=1.83\n",
            "[873 | 2160.84] loss=1.79 avg=1.83\n",
            "[874 | 2163.23] loss=2.18 avg=1.83\n",
            "[875 | 2165.62] loss=1.69 avg=1.83\n",
            "[876 | 2167.99] loss=1.52 avg=1.83\n",
            "[877 | 2170.36] loss=1.86 avg=1.83\n",
            "[878 | 2172.75] loss=1.48 avg=1.83\n",
            "[879 | 2175.13] loss=1.54 avg=1.82\n",
            "[880 | 2177.52] loss=1.52 avg=1.82\n",
            "[881 | 2179.89] loss=1.98 avg=1.82\n",
            "[882 | 2182.27] loss=1.82 avg=1.82\n",
            "[883 | 2184.66] loss=1.69 avg=1.82\n",
            "[884 | 2187.05] loss=1.95 avg=1.82\n",
            "[885 | 2189.43] loss=2.02 avg=1.82\n",
            "[886 | 2191.82] loss=1.70 avg=1.82\n",
            "[887 | 2194.20] loss=1.69 avg=1.82\n",
            "[888 | 2196.59] loss=1.78 avg=1.82\n",
            "[889 | 2198.97] loss=1.73 avg=1.82\n",
            "[890 | 2201.36] loss=1.37 avg=1.82\n",
            "[891 | 2203.74] loss=1.70 avg=1.81\n",
            "[892 | 2206.12] loss=1.72 avg=1.81\n",
            "[893 | 2208.51] loss=1.71 avg=1.81\n",
            "[894 | 2210.89] loss=1.77 avg=1.81\n",
            "[895 | 2213.28] loss=1.55 avg=1.81\n",
            "[896 | 2215.66] loss=1.75 avg=1.81\n",
            "[897 | 2218.03] loss=1.70 avg=1.81\n",
            "[898 | 2220.42] loss=1.90 avg=1.81\n",
            "[899 | 2222.79] loss=1.99 avg=1.81\n",
            "[900 | 2225.18] loss=1.85 avg=1.81\n",
            "======== SAMPLE 1 ========\n",
            "etime'..\n",
            "\n",
            "\n",
            "In the case of Unicode, the name is usually a prefix of ASCII, but it makes it difficult to embed in your own custom\n",
            "literals.  But it is really the character encoding that has been the cause of all\n",
            "this trouble.\n",
            "If your language doesn’t use ASCII’s alphabet, Unicode does need to be consulted: it’s\n",
            "important that it’s properly encoded.\n",
            "\n",
            "\n",
            "For convenience, Python’s str.ascii() function returns a string\n",
            "containing the name of the Unicode character, for each character except the\n",
            "first digit.  If you’re lucky, it will be character-cell-encoded, so that the\n",
            "ASCII encoded string will read character strings like this:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Notes: \n",
            "\n",
            "a) If it is not the first character of the Unicode character set,\n",
            "returns a Unicode letter.\n",
            "b) As with ASCII letters, if this string contains an ASCII letter, the\n",
            "ASCII encoded string will be character-cell-encoded, so that it\n",
            "will use its character '0' as its first character, followed by character\n",
            "spaces.\n",
            "c) If this string contains no ASCII letter, it should not contain the non-ASCII\n",
            "letter.\n",
            "d) If there are no other ASCII letters in the string, the Unicode letter is used as\n",
            "the first character. If the first character of the string is not a valid\n",
            "ASCII letter, the string will be encoded Unicode.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.3.2.9. Unicode Literals\n",
            "Python’s ui library also contains some types of Python bytecode.  A wide range of\n",
            "types are supported:\n",
            "\n",
            "Regular and Number objects\n",
            "Numbers\n",
            "Floating Point Python objects\n",
            "Ellipsis\n",
            "String objects\n",
            "Characters\n",
            "Sequences\n",
            "Boolean values\n",
            "\n",
            "\n",
            "In addition, there are constants used to control the behavior of Python’s\n",
            "literals: __linenum__, __stderr__, and\n",
            "__str__.  __linenum__ is the interface to __linenum__, while __stderr__\n",
            "is the API to turn Python’s variables into single values and to\n",
            "define constant strings for all literals. See the following table for the\n",
            "syntax of the “__linenum__” constants.  __stderr__ is the interface to the\n",
            "__linenum__ constants specified by the __stderr__ function used for\n",
            "Python.\n",
            "The __linenum__ functions define a variable which defines the type\n",
            "of the bytecode that will write it.  When not defined, the value returned by\n",
            "__linenum__ is used as the actual bytecode, and the variable can not be used\n",
            "differentively.  When the __linenum__ function is an object, though, the\n",
            "type which is used gets assigned to the variable in turn, and by default, the\n",
            "__linenum__ object will not get NULL.\n",
            "Here is a complete list of Python literals for Unix and Windows:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LIMITED_VARARGS\n",
            "\n",
            "\n",
            "\n",
            "True\n",
            "\n",
            "\n",
            "\n",
            "False\n",
            "\n",
            "\n",
            "\n",
            "None\n",
            "\n",
            "\n",
            "\n",
            "None\n",
            "\n",
            "\n",
            "\n",
            "FalseAllowed values\n",
            "None\n",
            "\n",
            "\n",
            "\n",
            "FalseSpecial\n",
            "Boolean value\n",
            "\n",
            "\n",
            "(0, True)\n",
            "\n",
            "\n",
            "(1, False)\n",
            "None\n",
            "\n",
            "\n",
            "(0, True)\n",
            "\n",
            "\n",
            "(1, False)\n",
            "None\n",
            "\n",
            "\n",
            "(1, False)\n",
            "\n",
            "\n",
            "(2, True)\n",
            "The following constants are defined for Python 2.x:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Constant\n",
            "\n",
            "Name\n",
            "Notes\n",
            "\n",
            "\n",
            "\n",
            "stderr\n",
            "The interface of the __linenum__ constants to Python’s type definition.\n",
            "\n",
            "Stderr gives a “type-specific error” message when an invalid type definition\n",
            "is encountered:\n",
            ">>> m = PyType_New(\"int\", 7, \"integer\")  # create object using the wrong type\n",
            ">>> m.__linenum__()\n",
            "int                     \n",
            "1\n",
            ">>> m.__stderr__()\n",
            "False\n",
            ">>> int('37')                   \n",
            "1\n",
            ">>> int(3)                      \n",
            "1\n",
            "\n",
            "\n",
            "This is very handy when you need to create a new value, but don’t know if it’s\n",
            "relevant at the moment:\n",
            "# Create a Python object:\n",
            "# type\n",
            "int('3')\n",
            "Py_TYPE(m)\n",
            "m.__linenum__\n",
            "1\n",
            ">>> int(3)                    | 1\n",
            "4\n",
            "\n",
            "[901 | 2238.31] loss=1.75 avg=1.81\n",
            "[902 | 2240.71] loss=1.85 avg=1.81\n",
            "[903 | 2243.09] loss=1.84 avg=1.81\n",
            "[904 | 2245.47] loss=2.03 avg=1.81\n",
            "[905 | 2247.85] loss=1.45 avg=1.81\n",
            "[906 | 2250.24] loss=1.72 avg=1.81\n",
            "[907 | 2252.63] loss=1.69 avg=1.81\n",
            "[908 | 2255.02] loss=1.77 avg=1.81\n",
            "[909 | 2257.40] loss=1.94 avg=1.81\n",
            "[910 | 2259.78] loss=1.97 avg=1.81\n",
            "[911 | 2262.17] loss=1.77 avg=1.81\n",
            "[912 | 2264.56] loss=1.51 avg=1.81\n",
            "[913 | 2266.95] loss=1.39 avg=1.80\n",
            "[914 | 2269.33] loss=1.76 avg=1.80\n",
            "[915 | 2271.71] loss=1.80 avg=1.80\n",
            "[916 | 2274.09] loss=2.04 avg=1.80\n",
            "[917 | 2276.47] loss=1.74 avg=1.80\n",
            "[918 | 2278.85] loss=2.26 avg=1.81\n",
            "[919 | 2281.23] loss=1.72 avg=1.81\n",
            "[920 | 2283.62] loss=1.90 avg=1.81\n",
            "[921 | 2286.00] loss=1.81 avg=1.81\n",
            "[922 | 2288.39] loss=1.64 avg=1.81\n",
            "[923 | 2290.78] loss=1.84 avg=1.81\n",
            "[924 | 2293.16] loss=1.82 avg=1.81\n",
            "[925 | 2295.53] loss=1.82 avg=1.81\n",
            "[926 | 2297.92] loss=1.40 avg=1.80\n",
            "[927 | 2300.30] loss=1.73 avg=1.80\n",
            "[928 | 2302.70] loss=1.76 avg=1.80\n",
            "[929 | 2305.09] loss=1.72 avg=1.80\n",
            "[930 | 2307.48] loss=2.02 avg=1.80\n",
            "[931 | 2309.87] loss=1.34 avg=1.80\n",
            "[932 | 2312.25] loss=1.80 avg=1.80\n",
            "[933 | 2314.63] loss=1.72 avg=1.80\n",
            "[934 | 2317.02] loss=1.81 avg=1.80\n",
            "[935 | 2319.42] loss=1.54 avg=1.80\n",
            "[936 | 2321.81] loss=1.88 avg=1.80\n",
            "[937 | 2324.20] loss=1.80 avg=1.80\n",
            "[938 | 2326.59] loss=1.89 avg=1.80\n",
            "[939 | 2328.98] loss=1.59 avg=1.80\n",
            "[940 | 2331.37] loss=1.55 avg=1.79\n",
            "[941 | 2333.75] loss=1.97 avg=1.79\n",
            "[942 | 2336.14] loss=1.66 avg=1.79\n",
            "[943 | 2338.53] loss=1.49 avg=1.79\n",
            "[944 | 2340.90] loss=1.75 avg=1.79\n",
            "[945 | 2343.29] loss=2.04 avg=1.79\n",
            "[946 | 2345.68] loss=1.92 avg=1.79\n",
            "[947 | 2348.07] loss=1.98 avg=1.80\n",
            "[948 | 2350.46] loss=1.54 avg=1.79\n",
            "[949 | 2352.83] loss=1.55 avg=1.79\n",
            "[950 | 2355.22] loss=1.73 avg=1.79\n",
            "[951 | 2357.59] loss=1.82 avg=1.79\n",
            "[952 | 2359.99] loss=1.87 avg=1.79\n",
            "[953 | 2362.37] loss=1.82 avg=1.79\n",
            "[954 | 2364.75] loss=1.43 avg=1.79\n",
            "[955 | 2367.13] loss=1.58 avg=1.79\n",
            "[956 | 2369.52] loss=1.69 avg=1.78\n",
            "[957 | 2371.91] loss=1.54 avg=1.78\n",
            "[958 | 2374.30] loss=1.55 avg=1.78\n",
            "[959 | 2376.68] loss=1.71 avg=1.78\n",
            "[960 | 2379.07] loss=1.77 avg=1.78\n",
            "[961 | 2381.44] loss=1.73 avg=1.78\n",
            "[962 | 2383.82] loss=1.59 avg=1.78\n",
            "[963 | 2386.20] loss=1.64 avg=1.78\n",
            "[964 | 2388.60] loss=1.92 avg=1.78\n",
            "[965 | 2390.98] loss=1.31 avg=1.77\n",
            "[966 | 2393.37] loss=1.50 avg=1.77\n",
            "[967 | 2395.75] loss=1.76 avg=1.77\n",
            "[968 | 2398.14] loss=2.08 avg=1.77\n",
            "[969 | 2400.53] loss=1.94 avg=1.77\n",
            "[970 | 2402.92] loss=1.56 avg=1.77\n",
            "[971 | 2405.30] loss=1.71 avg=1.77\n",
            "[972 | 2407.69] loss=1.59 avg=1.77\n",
            "[973 | 2410.07] loss=1.55 avg=1.77\n",
            "[974 | 2412.45] loss=2.06 avg=1.77\n",
            "[975 | 2414.83] loss=1.55 avg=1.77\n",
            "[976 | 2417.22] loss=1.67 avg=1.77\n",
            "[977 | 2419.61] loss=1.84 avg=1.77\n",
            "[978 | 2421.99] loss=1.74 avg=1.77\n",
            "[979 | 2424.38] loss=1.61 avg=1.77\n",
            "[980 | 2426.76] loss=1.79 avg=1.77\n",
            "[981 | 2429.14] loss=1.76 avg=1.77\n",
            "[982 | 2431.52] loss=1.79 avg=1.77\n",
            "[983 | 2433.92] loss=1.52 avg=1.76\n",
            "[984 | 2436.30] loss=1.89 avg=1.77\n",
            "[985 | 2438.70] loss=1.97 avg=1.77\n",
            "[986 | 2441.07] loss=1.27 avg=1.76\n",
            "[987 | 2443.46] loss=1.62 avg=1.76\n",
            "[988 | 2445.84] loss=1.74 avg=1.76\n",
            "[989 | 2448.23] loss=1.78 avg=1.76\n",
            "[990 | 2450.61] loss=1.60 avg=1.76\n",
            "[991 | 2453.00] loss=1.80 avg=1.76\n",
            "[992 | 2455.38] loss=2.08 avg=1.76\n",
            "[993 | 2457.77] loss=1.78 avg=1.76\n",
            "[994 | 2460.16] loss=1.83 avg=1.76\n",
            "[995 | 2462.54] loss=2.16 avg=1.77\n",
            "[996 | 2464.93] loss=1.60 avg=1.77\n",
            "[997 | 2467.31] loss=1.61 avg=1.76\n",
            "[998 | 2469.70] loss=1.95 avg=1.77\n",
            "[999 | 2472.08] loss=1.87 avg=1.77\n",
            "[1000 | 2474.46] loss=1.75 avg=1.77\n",
            "Saving checkpoint/run1/model-1000\n",
            "[Snippets]\n",
            "[UsefulSnippets]\n",
            "[From the Snippets section]\n",
            "[Words]\n",
            "[Words From the Noun]\n",
            "[Words From the Noun]\n",
            "\n",
            "\n",
            "\n",
            "The first line of the Formatter class shows the line number of\n",
            "the first line of the header.  The second line of the Formatter class\n",
            "indicates the number of the first line of the header.\n",
            "The header is not displayed unless the field is set to a , and\n",
            "the field is a dictionary of strings.\n",
            "\n",
            "\n",
            "\n",
            "The first line of Formatter class is defined by\n",
            "Formatter.setfield(value, type)\n",
            "If type is a sequence of\n",
            "strings that contains a field named value, the field is\n",
            "modified by the setfield() method.  If type is a single\n",
            "string, the field is modified by the setfield() method.  For example:\n",
            ">>> class _Field:\n",
            "...     'fieldname' = '__class__'\n",
            "...     'value' = 2\n",
            "\n",
            "\n",
            "This class is documented in section Field objects.\n",
            "\n",
            "\n",
            "See also\n",
            "The field syntax is detailed in the section Field Objects.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Footnotes\n",
            "\n",
            "1\n",
            "The following are missing in Python 2.  Some of these are\n",
            "needed because of the nature of the syntactic sugar used in the language specification.\n",
            "\n",
            "\n",
            "2\n",
            "In Python 2.3 and later, the __annotations__ attribute has an\n",
            "additional value of None, which is used to indicate that\n",
            "this attribute is not set to None.  In Python 3, the __annotations__ attribute\n",
            "has an additional value of None, which is used to indicate that\n",
            "it is not set to None.  In Python 3.4, the __annotations__ attribute has\n",
            "a value of None, which is used to indicate that it is not set\n",
            "to None.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3\n",
            "In Python 3.4, the __annotations__ attribute has an additional value of\n",
            "None, which is used to indicate that __annotations.  In\n",
            "Python 3.4, the __annotations__ attribute has a value of None.  In\n",
            "Python 3.4.4, the __annotations__ attribute has a value of None.  In\n",
            "Python 3.4.5, the __annotations__ attribute has a value of None.\n",
            "In Python 3.4.5, the __annotations__ attribute has a value of None.  In\n",
            "Python 3.4.6, the __annotations__ attribute has a value of None.  In\n",
            "Python 3.4.6.5, the __annotations__ attribute has a value of None.  In\n",
            "Python 3.4.7, the __annotations__ attribute has a value of None.\n",
            "In Python 3.4.7.5, the __annotations__ attribute has a value of None.\n",
            "In Python 3.4.7.6, the __annotations__ attribute has a value of None.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            "In Python 3.3 and later, the __annotations__ attribute has a value of\n",
            "None, which is used to indicate that\n",
            "__annotations.  In Python 3.3, the __annotations__ attribute has\n",
            "a value of None.  In Python 3.4, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.4, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.4.5, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.5.5, the __annotations__\n",
            "attribute has a value of None.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5\n",
            "In Python 3.3 and later, the __annotations__ attribute has a value\n",
            "of None, which is used to indicate that\n",
            "__annotations.  In Python 3.4, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.5, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.5.6, the __annotations__\n",
            "attribute has a value of None.\n",
            "\n",
            "\n",
            "\n",
            "6\n",
            "In Python 3.3 and later, the __annotations__ attribute has a value of\n",
            "None, which is used to indicate that\n",
            "__annotations.  In Python 3.4, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.5, the __annotations__\n",
            "attribute has a value of None.\n",
            "\n",
            "\n",
            "7\n",
            "In Python 3.4 and later, the __annotations__ attribute has a value\n",
            "of None, which is used to indicate that\n",
            "__annotations.  In Python 3.4, the __annotations__\n",
            "attribute has a value of None.  In Python 3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsrQikbhZs7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1a828646-34d9-4813-a569-6ba0a67c4883"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEi_ic5KFTB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir ./drive/My\\ Drive/gpt2\n",
        "! cp -R ./samples/* ./drive/My\\ Drive/gpt2/samples/\n",
        "! cp -R ./checkpoint/*  ./drive/My\\ Drive/gpt2/checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78OIwsODGa4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}